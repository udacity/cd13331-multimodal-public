{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Let's have some fun with Gemini and the open-source models!\n",
    "\n",
    "First, make sure you have setup your credentials. Move to the *_parent folder_* and do this:\n",
    "\n",
    "```bash\n",
    "cp env.example .env\n",
    "```\n",
    "\n",
    "Then open .env in the same folder and insert your API key for Gemini, like this:\n",
    "\n",
    "```yaml\n",
    "GEMINI_API_KEY=[your key]\n",
    "...rest of the file\n",
    "```\n",
    "while leaving the rest untouched.\n",
    "\n",
    "Then we can load the credentials like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials for Gemini\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NOTE: since our .env file is in the parent directory, we load from\n",
    "# \"../.env\". In Linux and Mac, \"..\" means \"the parent folder of the current folder\"\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create some utility functions that we will use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_image_from_url(image_url: str) -> Image.Image:\n",
    "    \n",
    "    return Image.open(\n",
    "        BytesIO(\n",
    "            requests.get(image_url).content\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "def display_image_with_caption(image: Image.Image, caption: str):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(caption, wrap=True, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's prepare the data we are going to use:\n",
    "\n",
    "**NOTE**: because the image is generated randomly, if you run this cell multiple times you will get a different image every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This URL generates a random JPG image every time you call it!\n",
    "image_url = \"https://picsum.photos/400/300\"\n",
    "image = get_image_from_url(image_url)\n",
    "\n",
    "# The following is for huggingface\n",
    "def get_huggingface_messages(img: Image.Image, prompt: str):\n",
    "\n",
    "    # TODO: Fill in the right content with the format expected by huggingface\n",
    "    # chat models:\n",
    "    # [\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": [\n",
    "    #             {\"type\": \"image\", \"image\": img},\n",
    "    #             {\"type\": \"text\", \"text\": prompt},\n",
    "    #         ]\n",
    "    #     },\n",
    "    # ]\n",
    "    messages = ...\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image analysis with Gemini\n",
    "\n",
    "Let's use Gemini to list all objects that are present in our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# NOTE: here we could ask for a caption, instead we ask to list the objects in a\n",
    "# specific way\n",
    "prompt = \"List all objects that are present in this image as a comma-separated list.\"\n",
    "\n",
    "\n",
    "def analyze_image_with_gemini(img: Image.Image, prompt: str) -> str:\n",
    "    \n",
    "    # TODO: create a genai.Client instance using your API key\n",
    "    client = ...  # complete\n",
    "\n",
    "    # Set thinking to 0 to save some money and get a faster response\n",
    "    config = types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    "\n",
    "    # Limit size to at most 600x600 to limit the usage of tokens\n",
    "    img.thumbnail((600, 600))\n",
    "\n",
    "    # Format image for Gemini call\n",
    "    image_bytes = BytesIO()\n",
    "    img.save(image_bytes, format='JPEG')\n",
    "    image_part = types.Part.from_bytes(\n",
    "        data=image_bytes.getvalue(), \n",
    "        mime_type='image/jpeg'\n",
    "    )\n",
    "\n",
    "    # TODO: call Gemini. Use the model \"gemini-2.5-flash-lite\" and pass in as contents\n",
    "    # your prompt and the image_part we have just created, as well as the config\n",
    "    response = client.models.generate_content(\n",
    "        model=...,  # complete\n",
    "        contents=[...], # complete\n",
    "        config=... # complete\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "caption = analyze_image_with_gemini(\n",
    "    image,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "display_image_with_caption(image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the prompt and see all the things you can ask Gemini to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmolVLM2 on Hugging Face\n",
    "\n",
    "Now let's do the same, but this time we use an open-source model locally (no external API call). We use one of the best small Visual Language Models, SmolVLM2. It fits confortably in our T4 GPU and it can do a lot of interesting things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\"\n",
    "\n",
    "# TODO: create an instance of the AutoProcessor for the model\n",
    "# specified in model_path using AutoProcessor.from_pretrained\n",
    "processor = ... # complete\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "# TODO: Create an instance of the model using the .from_pretrained\n",
    "# method of the AutoModelForImageTextToText class.\n",
    "# Looking at the documentation for this model, we must add the\n",
    "# following keywords as well: \"torch_dtype=torch.float16\" and\n",
    "# device_map=\"auto\"\n",
    "model = ...  # complete\n",
    "\n",
    "# This is taken from the documentation of the model, some\n",
    "# options are required some are optional\n",
    "inputs = processor.apply_chat_template(\n",
    "    # TODO: get the messages template for huggingface using the\n",
    "    # get_huggingface_messages function we defined above\n",
    "    ...,  # complete\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "# This takes around 10 seconds on GPU, around 4 minutes on a CPU\n",
    "# TODO: use the model by calling the .generate method and passing\n",
    "# the inputs we have created (use `**inputs`). Use also max_new_tokens\n",
    "# to limit the output and the time it takes to generate it. Start with 16\n",
    "# and increase it if you see that the output is truncated\n",
    "generated_ids = model.generate(\n",
    "    ...,  # complete \n",
    "    # If the output gets trucnated, try increasing this\n",
    "    max_new_tokens=..., # complete\n",
    ")\n",
    "\n",
    "# TODO: decode the output from the model to transform it back\n",
    "# to text\n",
    "generated_texts = processor.batch_decode(\n",
    "    # Remember to slice to avoid decoding your input prompt\n",
    "    # You can use \":, inputs['input_ids'].shape[1] : \" to slice\n",
    "    # generated_ids\n",
    "    ...,  # complete\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "result = generated_texts[0]\n",
    "\n",
    "display_image_with_caption(image, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio processing with Gemini\n",
    "\n",
    "Now let's see what Gemini can do with audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "prompt = \"Transcribe this audio file.\"\n",
    "\n",
    "\n",
    "def analyze_audio_with_gemini(audio_bytes: bytes, prompt: str) -> str:\n",
    "    \n",
    "    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        # Set thinking to 0 to save some money and get a faster response\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    "\n",
    "    # Format audio for Gemini call\n",
    "    # Earlier we had:\n",
    "    # image_part = types.Part.from_bytes(data=image_bytes.getvalue(), mime_type='image/jpeg')\n",
    "    # to get an image part, now we get an audio part in a similar way:\n",
    "    audio_part = types.Part.from_bytes(data=audio_bytes, mime_type='audio/wav')\n",
    "\n",
    "    # Call Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        # TODO: here pass in your prompt and the audio part instead of the image part as we did\n",
    "        # before. Everything else stays the same\n",
    "        contents=[...], # complete\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Example usage with WAV file bytes\n",
    "# Assuming you have audio_bytes containing WAV file data\n",
    "audio_bytes = open(\"../LJ025-0076.wav\", \"rb\").read()\n",
    "caption = analyze_audio_with_gemini(\n",
    "    audio_bytes,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "display(Audio(\"../LJ025-0076.wav\"))\n",
    "\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio processing with Whisper\n",
    "\n",
    "And now let's do the same with an open-source specialized audio model, Whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# This takes a few seconds on a T4 GPU and around 1 m on CPU\n",
    "# TODO: here we use the \"pipeline\" system of huggingface, available\n",
    "# from some pre-defined tasks, that simplifies all the steps and reduce\n",
    "# the boilerplate we need to write.\n",
    "# Use the model we already loaded, processor.tokenizer as tokenizer,\n",
    "# processor.feature_extractor as feature_extractor\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=...,  # complete\n",
    "    tokenizer=...,  # complete\n",
    "    feature_extractor=..., # complete\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# NOTE: this requires ffmpeg to be installed in your system\n",
    "# This is the text spoken in this file:\n",
    "# Many animals of even complex structure which live parasitically within others are wholly devoid of an alimentary cavity.\n",
    "result: dict = pipe(\"../LJ025-0076.wav\")\n",
    "\n",
    "print(\"==============\")\n",
    "print(result.get('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-showcase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
