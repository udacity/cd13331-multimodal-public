{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fan On/Off Detection using CLAP\n",
    "\n",
    "Let's put some of the things we learned to practice! In this notebook we are going to use CLAP zero-shot classification to measure when a fan is on and off. Let's get into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MP3 file\n",
    "audio_path = \"fan.mp3\"\n",
    "\n",
    "# TODO: load the audio file with librosa. Use a sample rate of 48000 Hz\n",
    "# HINT: use librosa.load, and set sr=48000\n",
    "audio, sr = ... # complete\n",
    "\n",
    "print(f\"Audio duration: {len(audio)/sr:.2f} seconds\")\n",
    "print(f\"Sample rate: {sr} Hz\")\n",
    "Audio(\n",
    "    data=audio,\n",
    "    rate=sr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup CLAP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize zero-shot audio classification pipeline\n",
    "# TODO: setup the zero-shot-audio-classification pipeline.\n",
    "# Use the \"laion/larger_clap_general\" model\n",
    "# HINT: use pipeline(). Provide the pipeline kind (zero-shot-audio-classification)\n",
    "# and the model (laion/larger_clap_general)\n",
    "pipe = ... # complete\n",
    "\n",
    "# TODO: Define candidate labels for the states of the fan (on and off)\n",
    "# You can use whatever you want, but in general it's better to be specific\n",
    "# and short.\n",
    "# You need one label for \"silence or background noise\" and one for \"the sound of a fan\"\n",
    "candidate_labels = ... #complete\n",
    "\n",
    "# TODO: whatever you chose as your label for the ON state in the previous line, set it here\n",
    "# (for example, \"the sound of a fan\")\n",
    "POSITIVE_LABEL = ... #complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Audio into Chunks\n",
    "\n",
    "Here we divide the audio in overlapping chunks of 1 second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "chunk_duration = 1.0  # seconds\n",
    "overlap = 0.25  # 50% overlap\n",
    "\n",
    "# Convert to samples\n",
    "chunk_samples = int(chunk_duration * sr)\n",
    "hop_samples = int(chunk_samples * (1 - overlap))\n",
    "\n",
    "# Create chunks and keep track of their start times\n",
    "chunks = []\n",
    "timestamps = []\n",
    "\n",
    "for start in range(0, len(audio) - chunk_samples + 1, hop_samples):\n",
    "    end = start + chunk_samples\n",
    "    chunk = audio[start:end]\n",
    "    timestamp = start / sr\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    timestamps.append(timestamp)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks of {chunk_duration}s each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's listen to one to make sure we did not do anything wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\n",
    "    data=chunks[2],\n",
    "    rate=sr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Each Chunk\n",
    "\n",
    "Now we classify each chunk to understand whether the fan is on or off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each chunk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "fan_scores = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(chunks), total=len(chunks)):\n",
    "\n",
    "    # TODO: use the pipeline we defined above to classify the chunk\n",
    "    # HINT: provide the chunk, and then set candidate_labels to (well)\n",
    "    # candidate_labels\n",
    "    result = ... #complete\n",
    "\n",
    "    # From the result we extract the probability of the POSITIVE_LABEL\n",
    "    prob_on = [x[\"score\"] for x in result if x[\"label\"] == POSITIVE_LABEL][0]\n",
    "\n",
    "    # Let's append it to our lists\n",
    "    fan_scores.append(prob_on)\n",
    "\n",
    "print(\"Classification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect State Changes\n",
    "\n",
    "Here we just detect when there are transitions ON/OFF or OFF/ON, and when they are. The idea is simple: whenever the probability for the positive class goes below 0.5, it means that the fan is OFF. On the opposite, every time the probability for the positive class goes from below to above 0.5, the fan has turned ON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fan scores to binary states (fan on/off)\n",
    "threshold = 0.5\n",
    "fan_states = [score > threshold for score in fan_scores]\n",
    "\n",
    "# Find state change points\n",
    "state_changes = []\n",
    "for i in range(1, len(fan_states)):\n",
    "    if fan_states[i] != fan_states[i-1]:\n",
    "        timestamp = timestamps[i]\n",
    "        new_state = \"ON\" if fan_states[i] else \"OFF\"\n",
    "        state_changes.append((timestamp, new_state))\n",
    "\n",
    "print(\"\\nDetected state changes:\")\n",
    "for timestamp, state in state_changes:\n",
    "    print(f\"Time: {timestamp:.2f}s - Fan: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's put everything together in a nice plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Waveform\n",
    "time_axis = np.linspace(0, len(audio)/sr, len(audio))\n",
    "ax1.plot(time_axis, audio)\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_title('Audio Waveform')\n",
    "\n",
    "# Fan scores\n",
    "ax2.plot(timestamps, fan_scores, 'b-', linewidth=2)\n",
    "ax2.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "ax2.set_ylabel('Fan Score')\n",
    "ax2.set_title('Fan Detection Score')\n",
    "ax2.legend()\n",
    "\n",
    "# Binary states\n",
    "ax3.plot(timestamps, fan_states, 'g-', linewidth=2)\n",
    "ax3.set_ylabel('Fan State')\n",
    "ax3.set_title('Fan On/Off State')\n",
    "ax3.set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Mel spectrogram\n",
    "mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "# Set extent to match the time axis of other plots\n",
    "extent = [0, len(audio)/sr, 0, 128]\n",
    "img = ax4.imshow(mel_spec_db, aspect='auto', origin='lower', extent=extent, cmap='viridis')\n",
    "ax4.set_ylabel('Mel Frequency Bins')\n",
    "ax4.set_xlabel('Time (seconds)')\n",
    "ax4.set_title('Mel Spectrogram')\n",
    "\n",
    "# Mark state changes\n",
    "for timestamp, state in state_changes:\n",
    "    color = 'green' if state == 'ON' else 'red'\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.axvline(x=timestamp, color=color, linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "total_on_time = sum(fan_states) * chunk_duration * (1 - overlap)\n",
    "total_duration = len(audio) / sr\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total audio duration: {total_duration:.2f} seconds\")\n",
    "print(f\"Estimated fan on time: {total_on_time:.2f} seconds\")\n",
    "print(f\"Fan duty cycle: {total_on_time/total_duration*100:.1f}%\")\n",
    "print(f\"Number of state changes: {len(state_changes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this is not the only way to achieve this result, and also probably not the most efficient. However, by exploiting the zero-shot capabilities of CLAP we were able to do this very quickly and without any training or optimization! \n",
    "\n",
    "Note however that in a real-world scenario we would need to test this solution thoroughly before deploying it in the field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-clap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
