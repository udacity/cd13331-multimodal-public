{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Search with CLIP4CLIP\n",
    "\n",
    "In this demo, we'll build a video search system using CLIP4CLIP, a model that can understand both video content and text descriptions. CLIP4CLIP extends the original CLIP model to work with video sequences by processing multiple frames and creating video embeddings.\n",
    "\n",
    "## What you'll learn:\n",
    "- Convert videos to embeddings using CLIP4CLIP\n",
    "- Search videos using text queries\n",
    "- Compare video similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries for video processing and the CLIP4CLIP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing Function\n",
    "\n",
    "This function converts a video file into a tensor of processed frames that CLIP4CLIP can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(size, n_px):\n",
    "        \n",
    "    return Compose([\n",
    "        Resize(size, interpolation=InterpolationMode.BICUBIC),            \n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])(n_px)\n",
    "\n",
    "\n",
    "def video2frames(video_path, frame_rate=1.0, size=224):\n",
    "    \"\"\"Convert video to preprocessed frames tensor.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)\n",
    "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    if fps < 1:\n",
    "        raise IOError(f\"ERROR: problem reading video file: {video_path}. No frames found.\")\n",
    "    \n",
    "    total_duration = (frameCount + fps - 1) // fps\n",
    "    start_sec, end_sec = 0, total_duration\n",
    "    interval = fps / frame_rate\n",
    "    frames_idx = np.floor(np.arange(start_sec*fps, end_sec*fps, interval))\n",
    "    \n",
    "    images = np.zeros([len(frames_idx), 3, size, size], dtype=np.float32)\n",
    "    last_frame = 0\n",
    "        \n",
    "    for i, idx in enumerate(frames_idx):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()    \n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)             \n",
    "        last_frame = i\n",
    "        images[i,:,:,:] = preprocess_frame(size, Image.fromarray(frame))\n",
    "    \n",
    "    # If we exited early the loop, truncate to the frames we actually\n",
    "    # have read (to avoid black frames at the end)\n",
    "    images = images[:last_frame+1]\n",
    "    cap.release()\n",
    "    \n",
    "    return torch.tensor(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLIP4CLIP Models\n",
    "\n",
    "We load clip4clip twice, once as a video encoder, once as a text encoder. Note that the model is the same, because clip4clip brings both text and videos to the same embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Searchium-ai/clip4clip-webvid150k\"\n",
    "\n",
    "# Load the vision model using CLIPVisionModelWithProjection\n",
    "vision_model = CLIPVisionModelWithProjection.from_pretrained(model_name)\n",
    "vision_model = vision_model.eval()\n",
    "\n",
    "# Load the text model using CLIPTextModelWithProjection\n",
    "text_model = CLIPTextModelWithProjection.from_pretrained(model_name)\n",
    "text_model = text_model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Embedding Function\n",
    "\n",
    "Create a function that converts video frames into a single embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_embedding(video_frames):\n",
    "    \"\"\"Convert video frames to a normalized embedding vector.\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get visual output from the vision model\n",
    "        visual_output = vision_model(video_frames)\n",
    "        \n",
    "        # Extract embeddings and normalize at the frame level\n",
    "        # (to bring all frames to weight equally)\n",
    "        embeddings = visual_output[\"image_embeds\"]\n",
    "        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate mean across all frames to get single video embedding\n",
    "        video_embedding = torch.mean(embeddings, dim=0)\n",
    "\n",
    "        # Normalize the final embedding\n",
    "        video_embedding = video_embedding / video_embedding.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "    return video_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Function\n",
    "\n",
    "Create a function that converts text queries into embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text_query):\n",
    "    \"\"\"Convert text query to normalized embedding vector.\"\"\"\n",
    "    \n",
    "    # Tokenize the text query\n",
    "    inputs = tokenizer([text_query], padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings from the text model\n",
    "        text_output = text_model(**inputs)\n",
    "        text_embedding = text_output.text_embeds\n",
    "\n",
    "        # Normalize the embedding\n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "    return text_embedding.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Videos\n",
    "\n",
    "Create a list of video file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 video files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos/bags.mp4\" controls  width=\"224\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos/cross.mp4\" controls  width=\"224\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos/dog.mp4\" controls  width=\"224\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from IPython.display import display, Video\n",
    "\n",
    "video_paths = glob.glob(\"videos/*\")\n",
    "\n",
    "print(f\"Found {len(video_paths)} video files\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    display(Video(video_path, width=224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Video Embeddings Database\n",
    "\n",
    "Let's process all videos and create embeddings for searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos and creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 3 videos\n",
      "Embeddings shape: torch.Size([3, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_embeddings = []\n",
    "\n",
    "print(\"Processing videos and creating embeddings...\")\n",
    "\n",
    "for video_path in tqdm(video_paths):\n",
    "    try:\n",
    "        # Convert video to frames\n",
    "        frames = video2frames(video_path)\n",
    "\n",
    "        # Get video embedding\n",
    "        embedding = get_video_embedding(frames)\n",
    "        \n",
    "        video_embeddings.append(embedding)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Stack all embeddings into a tensor\n",
    "video_embeddings_tensor = torch.stack(video_embeddings)\n",
    "\n",
    "print(f\"Created embeddings for {len(video_embeddings)} videos\")\n",
    "print(f\"Embeddings shape: {video_embeddings_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Search Function\n",
    "\n",
    "Let's implement the search function that finds videos most similar to a text query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_videos(query_text, video_embeddings_tensor, video_paths, top_k=3):\n",
    "    \"\"\"Search for videos using text query.\"\"\"\n",
    "    \n",
    "    # Get text embedding for the query\n",
    "    text_embedding = get_text_embedding(query_text)\n",
    "\n",
    "    # Calculate cosine similarity between text and all video embeddings\n",
    "    # Since embeddings are normalized, cosine similarity = dot product\n",
    "    similarities = torch.matmul(text_embedding, video_embeddings_tensor.T)\n",
    "\n",
    "    # Get top-k most similar videos\n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    top_scores = similarities[top_indices]\n",
    "    \n",
    "    print(f\"Search results for: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    for i, (idx, score) in enumerate(zip(top_indices, top_scores)):\n",
    "        video_path = video_paths[idx]\n",
    "        results.append((video_path, score.item()))\n",
    "        print(f\"{i+1}. {video_path} (similarity: {score:.3f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Video Search\n",
    "\n",
    "Now let's test our video search system with different text queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Search results for: 'animal playing'\n",
      "--------------------------------------------------\n",
      "1. videos/dog.mp4 (similarity: 0.233)\n",
      "2. videos/cross.mp4 (similarity: 0.163)\n",
      "3. videos/bags.mp4 (similarity: 0.128)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Search results for: 'I love motor sports!'\n",
      "--------------------------------------------------\n",
      "1. videos/cross.mp4 (similarity: 0.220)\n",
      "2. videos/dog.mp4 (similarity: 0.206)\n",
      "3. videos/bags.mp4 (similarity: 0.159)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Search results for: 'airport life'\n",
      "--------------------------------------------------\n",
      "1. videos/bags.mp4 (similarity: 0.300)\n",
      "2. videos/cross.mp4 (similarity: 0.160)\n",
      "3. videos/dog.mp4 (similarity: 0.150)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Try different search queries\n",
    "queries = [\n",
    "    \"animal playing\",\n",
    "    \"I love motor sports!\",\n",
    "    \"airport life\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    results = search_videos(query, video_embeddings_tensor, video_paths, top_k=3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "When implementing a video search system, there are a lot of choices to be made regarding the embeddings (embedding model, frame sampling strategy, similarity metrics...). Moreover, it is common for real system to include hybrid searches (metadata + embeddings), searches on multiple embeddings, as well as additional steps like re-ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-clip4clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
