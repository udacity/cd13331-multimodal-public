{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Video Salient Moments Detection with Pydantic AI and Gemini\n",
    "\n",
    "In this exercise you will build a system to detect salient moments in videos, based on Google Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Environment\n",
    "\n",
    "Before starting, make sure you have placed your Google Gemini credentials in the `.env` file:\n",
    "\n",
    "```bash\n",
    "cp env.example .env\n",
    "```\n",
    "then edit `.env` and modify GEMINI_API_KEY with your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent, BinaryContent\n",
    "from IPython.display import Video, display\n",
    "from typing import List, Optional\n",
    "\n",
    "# Load environment variables\n",
    "assert load_dotenv(), \"Please prepare a .env file with your GEMINI_API_KEY\"\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"GEMINI_API_KEY not found in .env file\"\n",
    "\n",
    "# This is needed to use asyncio within jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Response Schema\n",
    "\n",
    "We'll use Pydantic models to structure our salient moments detection. This structured output makes it easy to create video highlights, generate thumbnails at key moments, or build interactive video players with chapter markers.\n",
    "\n",
    "In this exercise we will build a nested schema, where a video-level pydantic object contains a list of SalientMoment objects, where each salient moment is described with its own schema. Let's do it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "\n",
    "# TODO: Define a SalientMoment schema (aka data model) with the following fields:\n",
    "# - timestamp: str - Approximate timestamp when this moment occurs\n",
    "# - moment_type: Literal[...] - Category of salient moment. A classification from\n",
    "#   \"action_peak\", \"emotional_highlight\", \"visual_transition\", \"climax\", \"scene_change\", \"other\"\n",
    "# - description: str - Clear description of what makes this moment significant\n",
    "# - visual_cues: List[str] - Visual elements that make this moment stand out\n",
    "class SalientMoment(BaseModel):\n",
    "    \"\"\"A single salient moment in the video.\"\"\"\n",
    "\n",
    "    ... #complete\n",
    "\n",
    "\n",
    "# TODO: This is now the video-level schema that contains a list of SalientMoment objects.\n",
    "class VideoSalientMoments(BaseModel):\n",
    "    \"\"\"Complete analysis of salient moments in a video.\"\"\"\n",
    "\n",
    "    video_summary: str = Field(\n",
    "        description=\"Brief 1-2 sentence summary of the overall video content\"\n",
    "    )\n",
    "    \n",
    "    total_moments_found: int = Field(\n",
    "        description=\"Number of salient moments identified\"\n",
    "    )\n",
    "    \n",
    "    # TODO: make `moments` a List of SalientMoment objects, and use \n",
    "    # Field(...) to add an appropriate description.\n",
    "    # HINT: use List[SalientMoment] as type\n",
    "    moments: ... #complete\n",
    "    \n",
    "    most_significant_moment: Optional[str] = Field(\n",
    "        description=\"Timestamp of the single most important moment, if one stands out\"\n",
    "    )\n",
    "    \n",
    "    pacing_analysis: str = Field(\n",
    "        description=\"Brief analysis of how the salient moments are distributed throughout the video\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pydantic AI Agent\n",
    "\n",
    "Set up the agent with Gemini for salient moments detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.google import GoogleModelSettings\n",
    "\n",
    "\n",
    "# Remove thinking to avoid long delays and timeouts\n",
    "settings = GoogleModelSettings(\n",
    "    google_thinking_config={\"thinking_budget\": 0},\n",
    ")\n",
    "\n",
    "# TODO: set up the agent. Make sure to set:\n",
    "# - model to \"gemini-2.5-flash-lite\"\n",
    "# - output_type to VideoSalientMoments\n",
    "# - instructions to guide the model to identify salient moments as per the exercise description.\n",
    "#.  We provide an example, but feel free to play with it!\n",
    "# - model_settings to the settings defined above\n",
    "moments_agent = Agent(\n",
    "    model=..., #complete\n",
    "    output_type=..., #complete\n",
    "    instructions=\"\"\"\n",
    "    You are an expert video analyst specializing in identifying salient moments. \n",
    "    Watch the video carefully and identify:\n",
    "    \n",
    "    1. Key moments that would be most interesting to viewers\n",
    "    2. Points where significant changes occur (visual, emotional, narrative)\n",
    "    3. Peaks of action, emotion, or visual interest\n",
    "    4. Important transitions or reveals\n",
    "    5. Moments that would make good thumbnails or highlights\n",
    "    \n",
    "    Focus on moments that:\n",
    "    - Capture viewer attention\n",
    "    - Represent turning points\n",
    "    - Show peak action or emotion\n",
    "    - Have strong visual impact\n",
    "    - Advance the story or message\n",
    "    \n",
    "    Provide approximate timestamps and be specific about why each moment is salient.\n",
    "    \"\"\",\n",
    "    model_settings=... #complete\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Video Processing\n",
    "\n",
    "Let's define a helper function to load and format video files for salient moments analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def load_video_for_moments_analysis(video_path):\n",
    "    \"\"\"Load video file and format it for Pydantic AI.\"\"\"\n",
    "    with open(video_path, 'rb') as f:\n",
    "        video_bytes = f.read()\n",
    "    \n",
    "    # Create binary content for Pydantic AI\n",
    "    video_content = BinaryContent(\n",
    "        data=video_bytes,\n",
    "        media_type='video/mp4'  # Adjust based on your video format\n",
    "    )\n",
    "    \n",
    "    return video_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Analysis\n",
    "\n",
    "Analyze a video to identify its salient moments. Use the `../cross.mp4` file that we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your video file path\n",
    "video_path = \"../cross.mp4\"  # Change this to your video file\n",
    "\n",
    "# Display the video in the notebook\n",
    "print(\"Original video:\")\n",
    "display(Video(video_path, width=400))\n",
    "\n",
    "# Load and analyze the video for salient moments\n",
    "video_content = load_video_for_moments_analysis(video_path)\n",
    "\n",
    "print(\"\\nAnalyzing salient moments...\")\n",
    "result = moments_agent.run_sync([\n",
    "    \"Identify and analyze the salient moments in this video.\", \n",
    "    video_content\n",
    "])\n",
    "\n",
    "print(\"\\nSalient Moments Analysis:\")\n",
    "pprint(result.output.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structured approach to salient moment detection makes it easy to build automated video editing tools, create dynamic thumbnails, or generate chapter markers for long-form content.\n",
    "\n",
    "For example, you could analyze a lot of footage of motocross and find salient moments like this one, then cut them out and mount them together in a highlight video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-salient-moments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
