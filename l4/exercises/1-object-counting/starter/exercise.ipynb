{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: object counting\n",
    "\n",
    "Let's apply what we learned about object counting in videos. We're going to count how many bags are passing through a conveyer belt at an airport.\n",
    "\n",
    "## Helpers\n",
    "\n",
    "In order for you to focus on the important bits, we define here for you some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from hydra import compose, initialize_config_module\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, Video\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "from yolo.tools.data_augmentation import PadAndResize\n",
    "from yolo.tools.solver import InferenceModel\n",
    "\n",
    "\n",
    "def get_fps_and_video_size(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Get frame size\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    return fps, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "def get_model_instance(input_video: str) -> tuple[InferenceModel, DictConfig]:\n",
    "\n",
    "    # Select device (use GPU if available)\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # This is necssary to avoid issues with tensors on different devices\n",
    "    # for this particular version of YOLO\n",
    "    torch.set_default_device(device)\n",
    "\n",
    "    # We load the default YOLO configuration, then we override some of its parameters\n",
    "    # (this is the hidiomatic way of doing things for Hydra, a configuration management tool)\n",
    "    with initialize_config_module(config_module=\"yolo.config\", version_base=None):\n",
    "        cfg = compose(\n",
    "            config_name=\"config\",\n",
    "            # These are the parameters we want to override\n",
    "            overrides=[\n",
    "                \"task.task=inference\",\n",
    "                # v9-s is the smallest model\n",
    "                \"model=v9-s\",\n",
    "                # We point to our video file\n",
    "                f\"task.data.source={input_video}\",\n",
    "                # We do not want to track on Weights and Biases\n",
    "                \"use_wandb=false\",\n",
    "                # We set out device\n",
    "                f\"device={device}\",\n",
    "            ],\n",
    "        )\n",
    "    # This is the way of loading and setting up a model\n",
    "    # with this version of YOLOv7\n",
    "    model = InferenceModel(cfg).to(device)\n",
    "    model.eval()\n",
    "    # This is a custom step that is necessary to setup the\n",
    "    # post-processing step of the model (which includes the \n",
    "    # Non-Maximum Suppression)\n",
    "    model.setup(cfg.task.task)\n",
    "\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "def preprocess_frame(\n",
    "    frame: np.ndarray,\n",
    "    pad_and_resize: PadAndResize,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor, Image.Image]:\n",
    "    # We need to pad and resize every frame to match the expected\n",
    "    # input resolution of the model\n",
    "\n",
    "    frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    untransformed_frame = frame.copy()\n",
    "\n",
    "    # PadAndResize can also operate on the ground truth boxes,\n",
    "    # which we don't have here (because this is inference on unknown data)\n",
    "    # So we use a dummy tensor\n",
    "    fake_boxes = torch.zeros((1, 6))\n",
    "    transformed_frame, _, transform_info = pad_and_resize(frame, fake_boxes)\n",
    "    transformed_frame = to_tensor(transformed_frame)\n",
    "    batch_of_one = transformed_frame[None]\n",
    "    rev_tensor = transform_info[None]\n",
    "\n",
    "    batch_of_one = batch_of_one.to(device)\n",
    "    rev_tensor = rev_tensor.to(device)\n",
    "\n",
    "    return batch_of_one, rev_tensor, untransformed_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call YOLO in the right way\n",
    "\n",
    "Here you will complete the first part. Look for the `TODO` comment in the `run_inference_on_one_frame` function code and complete the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "def run_inference_on_one_frame(\n",
    "    model: InferenceModel, frame: np.ndarray, pad_and_resize: Callable\n",
    ") -> list:\n",
    "    \n",
    "    # Pre-process the frame and get:\n",
    "    # the batch of one (the pre-processed frame ready to be fed to the model)\n",
    "    # the rev_tensor (the information needed to reverse the transformations)\n",
    "    # the untransformed_frame (the original frame, needed for visualization)\n",
    "    batch_of_one, rev_tensor, untransformed_frame = preprocess_frame(\n",
    "        frame, pad_and_resize, device=model.device\n",
    "    )\n",
    "\n",
    "    # TODO: Run YOLO. This will return the raw outputs of the model\n",
    "    # HINT: just call the model on `batch_of_one`\n",
    "    outputs = ... #complete\n",
    "\n",
    "    # TODO: Re-format outputs and apply Non-Maximum Suppression to remove\n",
    "    # duplicate detections\n",
    "    # HINT: use the model's `post_process` method on `outputs`,\n",
    "    # and remember to provide the inverse transformation `rev_tensor`\n",
    "    predicts = ... #complete\n",
    "\n",
    "    # We expect only one element in the batch (one frame)\n",
    "    assert len(predicts) == 1\n",
    "\n",
    "    return untransformed_frame, predicts[0].detach().cpu()\n",
    "\n",
    "\n",
    "def run_inference_on_video(\n",
    "    input_video: str\n",
    ") -> list:\n",
    "    \n",
    "    # Instance model\n",
    "    model, cfg = get_model_instance(input_video)\n",
    "\n",
    "    # We use opencv to loop through the frames of the video\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    # Get the total number of frames in the video\n",
    "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # We need to pad and resize every frame to match the expected\n",
    "    # input resolution of the model\n",
    "    pad_and_resize = PadAndResize(cfg.image_size)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # NOTE: this is absolutely necessary for good results with this\n",
    "        # version of YOLO. Failing to do this will result in very poor\n",
    "        # performance, because of the way the model has been trained.\n",
    "        with autocast(model.device.type):\n",
    "\n",
    "            for _ in tqdm(range(n_frames), total=n_frames):\n",
    "\n",
    "                # Read frame from the video\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if not ret:\n",
    "                    # Video is finished\n",
    "                    break\n",
    "\n",
    "                untransformed_frame, predicts = run_inference_on_one_frame(\n",
    "                    model, frame, pad_and_resize\n",
    "                )\n",
    "\n",
    "                # Append results for this frame\n",
    "                results.append([untransformed_frame, predicts])\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return results, cfg.dataset.class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it works by running it on our test video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"../bags.mp4\"\n",
    "# model, cfg = get_model_instance(input_video)\n",
    "# cfg.dataset.class_list\n",
    "results, class_list = run_inference_on_video(input_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's add the tracking part and the counting. Complete the lines with `TODO` in the following code:\n",
    "\n",
    "(Note that we do not use slicing here, as the objects in this video are large so there is no need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "\n",
    "class YOLOVideoObjectCounter:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the logic for counting objects in a video using YOLO and ByteTrack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_file: str,\n",
    "        line_zone: sv.LineZone = None,\n",
    "    ):\n",
    "\n",
    "        self.model, cfg = get_model_instance(video_file)\n",
    "\n",
    "        # Get FPS and video frame size\n",
    "        fps, video_frame_size = get_fps_and_video_size(video_file)\n",
    "\n",
    "        # TODO: create an instance of the tracker here using sv.ByteTrack\n",
    "        # Remember to set frame_rate to our video's fps\n",
    "        self.byte_tracker = ... #complete\n",
    "\n",
    "        self.line_zone = line_zone\n",
    "\n",
    "        # These are utilities to draw on the video for visualization\n",
    "        # purposes\n",
    "        self.line_zone_annotator = sv.LineZoneAnnotator(\n",
    "            thickness=2, text_thickness=2, text_scale=1\n",
    "        )\n",
    "        self.bounding_box_annotator = sv.BoxAnnotator()\n",
    "        self.label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "        # We need to pad and resize every frame to match the expected\n",
    "        # input resolution of the model\n",
    "        self.pad_and_resize = PadAndResize(cfg.image_size)\n",
    "\n",
    "        self.class_list = cfg.dataset.class_list\n",
    "\n",
    "    @staticmethod\n",
    "    def yolo_to_sv_detections(yolo_outputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Re-organize information in the format expected by the supervision tracker\n",
    "        \"\"\"\n",
    "\n",
    "        yolo_outputs = yolo_outputs.cpu().numpy()\n",
    "\n",
    "        detections = sv.Detections(\n",
    "            # yolo_outputs is a tensor of shape (n_detections, 6)\n",
    "            # where each detection is (class_id, x1, y1, x2, y2, score)\n",
    "            xyxy=yolo_outputs[:, 1:5],  # box coordinates\n",
    "            confidence=yolo_outputs[:, 5],  # confidence score\n",
    "            class_id=yolo_outputs[:, 0].astype(int),  # class id as integer\n",
    "        )\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def _yolo_inference(self, frame: np.ndarray) -> sv.Detections:\n",
    "        \"\"\"\n",
    "        Runs inference on one frame and returns results in the format\n",
    "        expected by the supervision tracker\n",
    "        \"\"\"\n",
    "        _, predicts = run_inference_on_one_frame(self.model, frame, self.pad_and_resize)\n",
    "        return self.yolo_to_sv_detections(predicts)\n",
    "\n",
    "    def run_on_one_frame(self, frame: np.ndarray, index: int) -> np.ndarray:\n",
    "\n",
    "        # TODO: Run YOLO on the frame\n",
    "        # HINT: use the `self._yolo_inference` method defined above on the frame\n",
    "        detections = ... #complete\n",
    "\n",
    "        # TODO: update the tracker with the new detections\n",
    "        # HINT: use the `update_with_detections` method of the tracker\n",
    "        detections = ... #complete\n",
    "\n",
    "        if self.line_zone is not None:\n",
    "            # Counting bags\n",
    "            # class_id 24 is backpack, 26 is handbag, class_id 28 is suitcase\n",
    "            bag_detections = detections[\n",
    "                (detections.class_id == 24)\n",
    "                | (detections.class_id == 26)\n",
    "                | (detections.class_id == 28)\n",
    "            ]\n",
    "            # TODO: trigger the line zone with the bag detections\n",
    "            # HINT: use the `trigger` method of self.line_zone on bag_detections\n",
    "            ... #complete\n",
    "\n",
    "        labels = [\n",
    "            f\"{self.class_list[int(class_id)]} {tracker_id} {confidence:0.2f}\"\n",
    "            for _, class_id, confidence, tracker_id in zip(\n",
    "                detections.xyxy,\n",
    "                detections.class_id,\n",
    "                detections.confidence,\n",
    "                detections.tracker_id,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        annotated_frame = self.bounding_box_annotator.annotate(\n",
    "            scene=frame.copy(), detections=detections\n",
    "        )\n",
    "\n",
    "        annotated_frame = self.label_annotator.annotate(\n",
    "            scene=annotated_frame, detections=detections, labels=labels\n",
    "        )\n",
    "\n",
    "        if self.line_zone is not None:\n",
    "            # Apply counting annotation to show the line and the\n",
    "            # counts\n",
    "            annotated_frame = self.line_zone_annotator.annotate(\n",
    "                annotated_frame, line_counter=self.line_zone\n",
    "            )\n",
    "\n",
    "        return annotated_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's test what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"../bags.mp4\"\n",
    "output_video = \"output_detected.m4v\"\n",
    "\n",
    "# Let's define a line in the video\n",
    "# We use a vertical line in the middle of the conveyor belt\n",
    "_, image_size = get_fps_and_video_size(input_video)\n",
    "START = sv.Point(image_size[0] // 2, 0)\n",
    "END = sv.Point(image_size[0] // 2, image_size[1])\n",
    "line_zone = sv.LineZone(\n",
    "    start=START, \n",
    "    end=END,\n",
    "    # We trigger the count when the center of the bounding\n",
    "    # box crosses the line\n",
    "    triggering_anchors=[sv.Position.CENTER],\n",
    ")\n",
    "\n",
    "# This works as before\n",
    "processor = YOLOVideoObjectCounter(video_file=input_video, line_zone=line_zone)\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=input_video,\n",
    "    target_path=output_video,\n",
    "    callback=processor.run_on_one_frame,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert from m4v to mp4 so we can display it here\n",
    "!ffmpeg -i {output_video} -c:v libx264 -tag:v avc1 bags_on_conveyor_belt_detected.mp4 -y > /dev/null 2>&1\n",
    "\n",
    "display(\n",
    "        Video(url=\"bags_on_conveyor_belt_detected.mp4\", embed=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-object-counting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
