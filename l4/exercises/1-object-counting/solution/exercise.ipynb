{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: object counting\n",
    "\n",
    "Let's apply what we learned about object counting in videos. We're going to count how many bags are passing through a conveyer belt at an airport.\n",
    "\n",
    "## Helpers\n",
    "\n",
    "In order for you to focus on the important bits, we define here for you some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from hydra import compose, initialize_config_module\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, Video\n",
    "\n",
    "from yolo.tools.data_augmentation import PadAndResize\n",
    "from yolo.tools.solver import InferenceModel\n",
    "\n",
    "\n",
    "def get_fps_and_video_size(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Get frame size\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    return fps, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "def get_model_instance(input_video: str) -> tuple[InferenceModel, DictConfig]:\n",
    "\n",
    "    # Select device (use GPU if available)\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # This is necssary to avoid issues with tensors on different devices\n",
    "    # for this particular version of YOLO\n",
    "    torch.set_default_device(device)\n",
    "\n",
    "    # We load the default YOLO configuration, then we override some of its parameters\n",
    "    # (this is the hidiomatic way of doing things for Hydra, a configuration management tool)\n",
    "    with initialize_config_module(config_module=\"yolo.config\", version_base=None):\n",
    "        cfg = compose(\n",
    "            config_name=\"config\",\n",
    "            # These are the parameters we want to override\n",
    "            overrides=[\n",
    "                \"task.task=inference\",\n",
    "                # v9-s is the smallest model\n",
    "                \"model=v9-s\",\n",
    "                # We point to our video file\n",
    "                f\"task.data.source={input_video}\",\n",
    "                # We do not want to track on Weights and Biases\n",
    "                \"use_wandb=false\",\n",
    "                # We set out device\n",
    "                f\"device={device}\",\n",
    "            ],\n",
    "        )\n",
    "    # This is the way of loading and setting up a model\n",
    "    # with this version of YOLOv7\n",
    "    model = InferenceModel(cfg).to(device)\n",
    "    model.eval()\n",
    "    # This is a custom step that is necessary to setup the\n",
    "    # post-processing step of the model (which includes the \n",
    "    # Non-Maximum Suppression)\n",
    "    model.setup(cfg.task.task)\n",
    "\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "def preprocess_frame(\n",
    "    frame: np.ndarray,\n",
    "    pad_and_resize: PadAndResize,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor, Image.Image]:\n",
    "    # We need to pad and resize every frame to match the expected\n",
    "    # input resolution of the model\n",
    "\n",
    "    frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    untransformed_frame = frame.copy()\n",
    "\n",
    "    # PadAndResize can also operate on the ground truth boxes,\n",
    "    # which we don't have here (because this is inference on unknown data)\n",
    "    # So we use a dummy tensor\n",
    "    fake_boxes = torch.zeros((1, 6))\n",
    "    transformed_frame, _, transform_info = pad_and_resize(frame, fake_boxes)\n",
    "    transformed_frame = to_tensor(transformed_frame)\n",
    "    batch_of_one = transformed_frame[None]\n",
    "    rev_tensor = transform_info[None]\n",
    "\n",
    "    batch_of_one = batch_of_one.to(device)\n",
    "    rev_tensor = rev_tensor.to(device)\n",
    "\n",
    "    return batch_of_one, rev_tensor, untransformed_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call YOLO in the right way\n",
    "\n",
    "Here you will complete the first part. Look for the `TODO` comment in the `run_inference_on_one_frame` function code and complete the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "def run_inference_on_one_frame(\n",
    "    model: InferenceModel, frame: np.ndarray, pad_and_resize: Callable\n",
    ") -> list:\n",
    "    \n",
    "    # Pre-process the frame and get:\n",
    "    # the batch of one (the pre-processed frame ready to be fed to the model)\n",
    "    # the rev_tensor (the information needed to reverse the transformations)\n",
    "    # the untransformed_frame (the original frame, needed for visualization)\n",
    "    batch_of_one, rev_tensor, untransformed_frame = preprocess_frame(\n",
    "        frame, pad_and_resize, device=model.device\n",
    "    )\n",
    "\n",
    "    # TODO: Run YOLO. This will return the raw outputs of the model\n",
    "    # HINT: just call the model on `batch_of_one`\n",
    "    outputs = model(batch_of_one)\n",
    "\n",
    "    # TODO: Re-format outputs and apply Non-Maximum Suppression to remove\n",
    "    # duplicate detections\n",
    "    # HINT: use the model's `post_process` method on `outputs`,\n",
    "    # and remember to provide the inverse transformation `rev_tensor`\n",
    "    predicts = model.post_process(outputs, rev_tensor=rev_tensor)\n",
    "\n",
    "    # We expect only one element in the batch (one frame)\n",
    "    assert len(predicts) == 1\n",
    "\n",
    "    return untransformed_frame, predicts[0].detach().cpu()\n",
    "\n",
    "\n",
    "def run_inference_on_video(\n",
    "    input_video: str\n",
    ") -> list:\n",
    "    \n",
    "    # Instance model\n",
    "    model, cfg = get_model_instance(input_video)\n",
    "\n",
    "    # We use opencv to loop through the frames of the video\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    # Get the total number of frames in the video\n",
    "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # We need to pad and resize every frame to match the expected\n",
    "    # input resolution of the model\n",
    "    pad_and_resize = PadAndResize(cfg.image_size)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # NOTE: this is absolutely necessary for good results with this\n",
    "        # version of YOLO. Failing to do this will result in very poor\n",
    "        # performance, because of the way the model has been trained.\n",
    "        with autocast(model.device.type):\n",
    "\n",
    "            for _ in tqdm(range(n_frames), total=n_frames):\n",
    "\n",
    "                # Read frame from the video\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if not ret:\n",
    "                    # Video is finished\n",
    "                    break\n",
    "\n",
    "                untransformed_frame, predicts = run_inference_on_one_frame(\n",
    "                    model, frame, pad_and_resize\n",
    "                )\n",
    "\n",
    "                # Append results for this frame\n",
    "                results.append([predicts])\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return results, cfg.dataset.class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it works by running it on our test video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/25 15:30:30] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> üöú Building YOLO                                                            <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/25 15:30:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m üöú Building YOLO                                                            \u001b]8;id=215674;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=950610;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building backbone                                                      <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building backbone                                                      \u001b]8;id=462811;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=230189;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building neck                                                          <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building neck                                                          \u001b]8;id=415750;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=345173;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building head                                                          <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building head                                                          \u001b]8;id=315829;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=87808;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building detection                                                     <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building detection                                                     \u001b]8;id=561963;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=269324;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building auxiliary                                                     <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building auxiliary                                                     \u001b]8;id=536368;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=240431;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/25 15:30:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> ‚úÖ Success load model &amp; weight                                             <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#189\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/25 15:30:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ‚úÖ Success load model & weight                                             \u001b]8;id=870926;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=133289;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#189\u001b\\\u001b[2m189\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> üß∏ Found no stride of model, performed a dummy test for      <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bounding_box_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py#346\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">346</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         auto-anchor size                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m üß∏ Found no stride of model, performed a dummy test for      \u001b]8;id=709894;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py\u001b\\\u001b[2mbounding_box_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=911612;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py#346\u001b\\\u001b[2m346\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         auto-anchor size                                             \u001b[2m                         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [01:00<00:00,  8.44it/s]\n"
     ]
    }
   ],
   "source": [
    "input_video = \"../bags.mp4\"\n",
    "# model, cfg = get_model_instance(input_video)\n",
    "# cfg.dataset.class_list\n",
    "results, class_list = run_inference_on_video(input_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's add the tracking part and the counting. Complete the lines with `TODO` in the following code:\n",
    "\n",
    "(Note that we do not use slicing here, as the objects in this video are large so there is no need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "\n",
    "class YOLOVideoObjectCounter:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the logic for counting objects in a video using YOLO and ByteTrack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_file: str,\n",
    "        line_zone: sv.LineZone = None,\n",
    "    ):\n",
    "\n",
    "        self.model, cfg = get_model_instance(video_file)\n",
    "\n",
    "        # Get FPS and video frame size\n",
    "        fps, video_frame_size = get_fps_and_video_size(video_file)\n",
    "\n",
    "        # TODO: create an instance of the tracker here.\n",
    "        # Remember to set frame_rate to our video's fps\n",
    "        self.byte_tracker = sv.ByteTrack(frame_rate=fps)\n",
    "\n",
    "        self.line_zone = line_zone\n",
    "\n",
    "        # These are utilities to draw on the video for visualization\n",
    "        # purposes\n",
    "        self.line_zone_annotator = sv.LineZoneAnnotator(\n",
    "            thickness=2, text_thickness=2, text_scale=1\n",
    "        )\n",
    "        self.bounding_box_annotator = sv.BoxAnnotator()\n",
    "        self.label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "        # We need to pad and resize every frame to match the expected\n",
    "        # input resolution of the model\n",
    "        self.pad_and_resize = PadAndResize(cfg.image_size)\n",
    "\n",
    "        self.class_list = cfg.dataset.class_list\n",
    "\n",
    "    @staticmethod\n",
    "    def yolo_to_sv_detections(yolo_outputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Re-organize information in the format expected by the supervision tracker\n",
    "        \"\"\"\n",
    "\n",
    "        yolo_outputs = yolo_outputs.cpu().numpy()\n",
    "\n",
    "        detections = sv.Detections(\n",
    "            # yolo_outputs is a tensor of shape (n_detections, 6)\n",
    "            # where each detection is (class_id, x1, y1, x2, y2, score)\n",
    "            xyxy=yolo_outputs[:, 1:5],  # box coordinates\n",
    "            confidence=yolo_outputs[:, 5],  # confidence score\n",
    "            class_id=yolo_outputs[:, 0].astype(int),  # class id as integer\n",
    "        )\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def _yolo_inference(self, frame: np.ndarray) -> sv.Detections:\n",
    "        \"\"\"\n",
    "        Runs inference on one frame and returns results in the format\n",
    "        expected by the supervision tracker\n",
    "        \"\"\"\n",
    "        _, predicts = run_inference_on_one_frame(self.model, frame, self.pad_and_resize)\n",
    "        return self.yolo_to_sv_detections(predicts)\n",
    "\n",
    "    def run_on_one_frame(self, frame: np.ndarray, index: int) -> np.ndarray:\n",
    "\n",
    "        # TODO: Run YOLO on the frame\n",
    "        # HINT: use the `_yolo_inference` method defined above\n",
    "        detections = self._yolo_inference(frame)\n",
    "\n",
    "        # TODO: update the tracker with the new detections\n",
    "        # HINT: use the `update_with_detections` method of the tracker\n",
    "        detections = self.byte_tracker.update_with_detections(detections)\n",
    "\n",
    "        if self.line_zone is not None:\n",
    "            # Counting bags\n",
    "            # class_id 24 is backpack, 26 is handbag, class_id 28 is suitcase\n",
    "            bag_detections = detections[\n",
    "                (detections.class_id == 24)\n",
    "                | (detections.class_id == 26)\n",
    "                | (detections.class_id == 28)\n",
    "            ]\n",
    "            # TODO: trigger the line zone with the bag detections\n",
    "            # HINT: use the `trigger` method of the line zone\n",
    "            self.line_zone.trigger(bag_detections)\n",
    "\n",
    "        labels = [\n",
    "            f\"{self.class_list[int(class_id)]} {tracker_id} {confidence:0.2f}\"\n",
    "            for _, class_id, confidence, tracker_id in zip(\n",
    "                detections.xyxy,\n",
    "                detections.class_id,\n",
    "                detections.confidence,\n",
    "                detections.tracker_id,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        annotated_frame = self.bounding_box_annotator.annotate(\n",
    "            scene=frame.copy(), detections=detections\n",
    "        )\n",
    "\n",
    "        annotated_frame = self.label_annotator.annotate(\n",
    "            scene=annotated_frame, detections=detections, labels=labels\n",
    "        )\n",
    "\n",
    "        if self.line_zone is not None:\n",
    "            # Apply counting annotation to show the line and the\n",
    "            # counts\n",
    "            annotated_frame = self.line_zone_annotator.annotate(\n",
    "                annotated_frame, line_counter=self.line_zone\n",
    "            )\n",
    "\n",
    "        return annotated_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's test what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/25 15:36:56] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> üöú Building YOLO                                                            <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/25 15:36:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m üöú Building YOLO                                                            \u001b]8;id=813435;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=957825;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building backbone                                                      <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building backbone                                                      \u001b]8;id=492958;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=442599;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building neck                                                          <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building neck                                                          \u001b]8;id=739960;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=585586;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building head                                                          <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building head                                                          \u001b]8;id=312219;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=429041;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building detection                                                     <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building detection                                                     \u001b]8;id=648583;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=935553;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>   üèó  Building auxiliary                                                     <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   üèó  Building auxiliary                                                     \u001b]8;id=684618;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=968080;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#38\u001b\\\u001b[2m38\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> ‚úÖ Success load model &amp; weight                                             <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">yolo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#189\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ‚úÖ Success load model & weight                                             \u001b]8;id=581029;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py\u001b\\\u001b[2myolo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=238242;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/model/yolo.py#189\u001b\\\u001b[2m189\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> üß∏ Found no stride of model, performed a dummy test for      <a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bounding_box_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py#346\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">346</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         auto-anchor size                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m üß∏ Found no stride of model, performed a dummy test for      \u001b]8;id=372811;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py\u001b\\\u001b[2mbounding_box_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=841581;file:///Users/giacomov/develop/cd13331-multimodal/l4/exercises/1-object-counting/.venv/lib/python3.12/site-packages/yolo/utils/bounding_box_utils.py#346\u001b\\\u001b[2m346\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         auto-anchor size                                             \u001b[2m                         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18345db515704f788b34d31d72b80ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_video = \"../bags.mp4\"\n",
    "output_video = \"output_detected.m4v\"\n",
    "\n",
    "# Let's define a line in the video\n",
    "# We use a vertical line in the middle of the conveyor belt\n",
    "_, image_size = get_fps_and_video_size(input_video)\n",
    "START = sv.Point(image_size[0] // 2, 0)\n",
    "END = sv.Point(image_size[0] // 2, image_size[1])\n",
    "line_zone = sv.LineZone(\n",
    "    start=START, \n",
    "    end=END,\n",
    "    # We trigger the count when the center of the bounding\n",
    "    # box crosses the line\n",
    "    triggering_anchors=[sv.Position.CENTER],\n",
    ")\n",
    "\n",
    "# This works as before\n",
    "processor = YOLOVideoObjectCounter(video_file=input_video, line_zone=line_zone)\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=input_video,\n",
    "    target_path=output_video,\n",
    "    callback=processor.run_on_one_frame,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"bags_on_conveyor_belt_detected.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Convert from m4v to mp4 so we can display it here\n",
    "!ffmpeg -i {output_video} -c:v libx264 -tag:v avc1 bags_on_conveyor_belt_detected.mp4 -y > /dev/null 2>&1\n",
    "\n",
    "display(\n",
    "        Video(url=\"bags_on_conveyor_belt_detected.mp4\", embed=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-object-counting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
