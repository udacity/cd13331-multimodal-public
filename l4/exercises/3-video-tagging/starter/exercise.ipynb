{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Video Tagging with Pydantic AI and Gemini\n",
    "\n",
    "In this exercise you will use Pydantic AI with Gemini 2.5 Flash to tag videos.\n",
    "\n",
    "We will pretend we have a small collection of videos we want to use to train a model for planning for a robotic hand. The hand needs to learn basic object manipulations. We want to use video tagging to pre-label and categorize our videos, so downstream Q/A teams can more easily assess them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Environment\n",
    "\n",
    "Before starting, make sure you have placed your Google Gemini credentials in the `.env` file:\n",
    "\n",
    "```bash\n",
    "cp env.example .env\n",
    "```\n",
    "then edit `.env` and modify GEMINI_API_KEY with your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent, BinaryContent\n",
    "from IPython.display import Video, display\n",
    "\n",
    "# Load environment variables\n",
    "assert load_dotenv(), \"Please prepare a .env file with your GEMINI_API_KEY\"\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"GEMINI_API_KEY not found in .env file\"\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# This is needed to use asyncio within jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Response Schema\n",
    "\n",
    "As usual, we need to define our response schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import Literal\n",
    "\n",
    "\n",
    "# TODO: Define a video tagging output schema with Pydantic AI\n",
    "# The schema should include:\n",
    "# 1. description: a detailed description of the action performed in the video\n",
    "# 2. An overall quality assessment (poor, ok, good)\n",
    "# 3. A list of actions (move sideways, move vertically,\n",
    "#    put object 1 into object 2, pull object 1 out of object 2)\n",
    "# 4. A list of objects present in the video from the following options:\n",
    "#    candle, container, cotton swabs, basket, box, tape, board game, mug,\n",
    "#    jar, slipper, container, scarf, smartphone, pen\n",
    "# HINT: remember to use Literal[...] to define fields that can only have specific values,\n",
    "# for example Literal[\"poor\", \"ok\", \"good\"]\n",
    "class VideoTagging(BaseModel):\n",
    "    \"\"\"Structured output for video tagging analysis.\"\"\"\n",
    "\n",
    "    # TODO: remember to assign the type and use the Field function to add \n",
    "    # a description\n",
    "    description: str = Field(\n",
    "        description=\"A detailed description of the action performed in the video\"\n",
    "    )\n",
    "    \n",
    "    # TODO\n",
    "    # HINT: remember you can use Literal[...] to restrict the possible values\n",
    "    quality: ... #complete\n",
    "\n",
    "    # TODO\n",
    "    actions: ... #complete\n",
    "\n",
    "    # TODO\n",
    "    objects_involved_in_the_action: int = Field(\n",
    "        description=\"Number of distinct objects involved in the action\"\n",
    "    )\n",
    "    \n",
    "    objects_present: List[\n",
    "        Literal[\n",
    "            \"candle\",\n",
    "            \"container\",\n",
    "            \"cotton swabs\",\n",
    "            \"basket\",\n",
    "            \"box\",\n",
    "            \"tape\",\n",
    "            \"board game\",\n",
    "            \"mug\",\n",
    "            \"jar\",\n",
    "            \"slipper\",\n",
    "            \"container\",\n",
    "            \"scarf\",\n",
    "            \"smartphone\",\n",
    "            \"pen\",\n",
    "        ]\n",
    "    ] = Field(\n",
    "        description=\"List of objects present in the video (e.g., person, car, tree)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pydantic AI Agent\n",
    "\n",
    "Set up the agent with Gemini for video content analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.google import GoogleModelSettings\n",
    "\n",
    "\n",
    "# Remove thinking to avoid long delays and timeouts\n",
    "settings = GoogleModelSettings(\n",
    "    google_thinking_config={\"thinking_budget\": 0},\n",
    "    temperature=0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# TODO: Create an agent for video tagging using the VideoTagging schema\n",
    "# Use the \"gemini-2.5-flash-lite\" model and the settings defined above\n",
    "# Remember to use output_type to specify the output schema to what we defined\n",
    "# above\n",
    "# Craft a short but precise set of instructions for the agent\n",
    "# Also set retries=5 or some large number, as some calls tend to fail because\n",
    "# the video is larger than just text\n",
    "video_agent = ... #complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Video Processing\n",
    "\n",
    "Let's define a helper function to load and format video files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_for_analysis(video_path):\n",
    "    \"\"\"Load video file and format it for Pydantic AI.\"\"\"\n",
    "    with open(video_path, 'rb') as f:\n",
    "        video_bytes = f.read()\n",
    "    \n",
    "    # Create binary content for Pydantic AI\n",
    "    # (see note at the end about using File API for longer videos)\n",
    "    video_content = BinaryContent(\n",
    "        data=video_bytes,\n",
    "        media_type='video/mp4'  # Adjust based on your video format\n",
    "    )\n",
    "    \n",
    "    return video_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Analysis\n",
    "\n",
    "Analyze a video file to see the structured description output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "videos = Path(\"../videos\").glob(\"*.mp4\")\n",
    "\n",
    "for video_path in videos:\n",
    "    print(f\"Original video: {video_path.name}\")\n",
    "    display(Video(str(video_path)))\n",
    "    \n",
    "    video_content = load_video_for_analysis(video_path)\n",
    "    \n",
    "    print(\"\\nAnalyzing video content...\")\n",
    "\n",
    "    # TODO: run the agent on the video\n",
    "    # HINT: just call video_agent.run_sync providing a list containing a prompt\n",
    "    # (like \"provide a description of this video\"), and the preprocessed video\n",
    "    # (video_content)\n",
    "    result = ... #complete\n",
    "    \n",
    "    print(\"\\nVideo Analysis Results:\")\n",
    "    pprint(result.output.model_dump(), width=80, depth=None)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-video-tagging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
