{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-labeling Pipeline for Driving Scenes\n",
    "\n",
    "In this notebook we bring everything together in a realistic (although simplistic) application: auto-labeling for self-driving cars. This is often the first level of data curation, typically followed by manual screening.\n",
    "\n",
    "We will:\n",
    "1. Classify images into the time of day as well as driving conditions\n",
    "2. Run object detection to find cars and trucks\n",
    "3. Segment the detections\n",
    "\n",
    "in one unified pipelines (our application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Let's start by defining some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Union, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw, Image\n",
    "import numpy as np\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "\n",
    "def get_distinct_colors(n_colors):\n",
    "    if n_colors <= 10:\n",
    "        cmap = plt.cm.tab10\n",
    "    elif n_colors <= 20:\n",
    "        cmap = plt.cm.tab20\n",
    "    else:\n",
    "        cmap = plt.cm.tab20\n",
    "        # Cycle through colors if more than 20\n",
    "\n",
    "    colors = [cmap(i % cmap.N) for i in range(n_colors)]\n",
    "    return colors\n",
    "\n",
    "\n",
    "def visualize_detr_detections(\n",
    "    image: Image.Image,\n",
    "    boxes: torch.Tensor,\n",
    "    scores: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    class_labels: List[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize DETR detections with bounding boxes, confidence scores, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Visualize detections\n",
    "    draw_image = image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image, mode='RGBA')\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        label_id = int(label.item())\n",
    "\n",
    "        # Draw bounding box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "\n",
    "        # Draw label and confidence score\n",
    "        class_name = class_labels[label_id]\n",
    "        text = f\"{class_name}: {score:.2f}\"\n",
    "        draw.text((x1, y1 - 20), text, fill=\"red\")\n",
    "\n",
    "    fig, sub = plt.subplots(figsize=(12, 8))\n",
    "    sub.imshow(draw_image)\n",
    "    sub.set_title(\"RT-DETR Car Detections\")\n",
    "    sub.axis(\"off\")\n",
    "\n",
    "    plt.show(fig)\n",
    "\n",
    "\n",
    "def visualize_pipeline_results(image, time_pred, time_conf, weather_pred, weather_conf, masks):\n",
    "    \"\"\"\n",
    "    Visualize complete pipeline results with classifications and segmentation masks.\n",
    "    \"\"\"\n",
    "    fig, sub = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "    # Show original image with slight transparency\n",
    "    sub.imshow(image, alpha=0.7)\n",
    "\n",
    "    # Overlay segmentation masks if any\n",
    "    if len(masks) > 0:\n",
    "        colors = get_distinct_colors(len(masks))\n",
    "\n",
    "        for i, mask in enumerate(masks):\n",
    "            colored_mask = np.zeros((*mask.shape, 4))\n",
    "            color_rgb = colors[i][:3]\n",
    "            colored_mask[mask > 0, :3] = color_rgb\n",
    "            colored_mask[mask > 0, 3] = 0.6\n",
    "            sub.imshow(colored_mask)\n",
    "\n",
    "    # Add classification results as text\n",
    "    title = f\"Auto-labeling Results\\n\" \\\n",
    "            f\"Time: {time_pred} ({time_conf:.2f}) | \" \\\n",
    "            f\"Weather: {weather_pred} ({weather_conf:.2f})\\n\" \\\n",
    "            f\"Objects detected and segmented: {len(masks)}\"\n",
    "    \n",
    "    sub.set_title(title, fontsize=12, pad=20)\n",
    "    sub.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Here we load all the models we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n",
    "from transformers import Sam2Processor, Sam2Model\n",
    "\n",
    "# Load CLIP model for scene classification\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load RT-DETR model trained on COCO\n",
    "detr_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "detr_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "\n",
    "# Load SAM2.1 model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam_model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-base-plus\").to(device)\n",
    "sam_processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-base-plus\")\n",
    "\n",
    "# Define classification labels\n",
    "TIME_OF_DAY_LABELS = [\"sunset or sunrise\", \"day\", \"night\"]\n",
    "WEATHER_LABELS = [\"normal\", \"rain\", \"fog\", \"clouds\", \"snow\"]\n",
    "\n",
    "print(\"All models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Pipeline Functions\n",
    "\n",
    "Now let's define the core functions for each step of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_driving_conditions(image: Image.Image) -> Dict[str, Union[str, float]]:\n",
    "\n",
    "    # TODO: Run clip_processor on the TIME_OF_DAY_LABELS and the input image\n",
    "    # Reminder: clip_processor(text=[your text], images=[your image], return_tensor=\"pt\", padding=True)\n",
    "    time_inputs = ... # complete\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # TODO: run the clip model on the pre-processed inputs\n",
    "        # HINT: remember to unpack **time_inputs when passing it to clip_model\n",
    "        time_outputs = ... # complete\n",
    "\n",
    "        # Extract probabilities for the different classes\n",
    "        time_logits = time_outputs.logits_per_image[0]\n",
    "        time_probs = torch.softmax(time_logits, dim=-1)\n",
    "    \n",
    "    time_prediction = TIME_OF_DAY_LABELS[torch.argmax(time_probs)]\n",
    "    time_confidence = torch.max(time_probs).item()\n",
    "\n",
    "    # Same for weather condition\n",
    "    weather_inputs = clip_processor(\n",
    "        text=WEATHER_LABELS, images=image, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weather_outputs = clip_model(**weather_inputs)\n",
    "        weather_logits = weather_outputs.logits_per_image[0]\n",
    "        weather_probs = torch.softmax(weather_logits, dim=-1)\n",
    "\n",
    "    weather_prediction = WEATHER_LABELS[torch.argmax(weather_probs)]\n",
    "    weather_confidence = torch.max(weather_probs).item()\n",
    "\n",
    "    return {\n",
    "        \"time_of_day\": time_prediction,\n",
    "        \"time_confidence\": time_confidence,\n",
    "        \"weather\": weather_prediction,\n",
    "        \"weather_confidence\": weather_confidence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vehicles_and_people(\n",
    "    image: Image.Image,\n",
    "    threshold: float = 0.3,\n",
    "    iou_threshold_for_dashboard_removal: float = 0.8,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Detect vehicles and people in image using RT-DETR.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image to process\n",
    "        threshold: Detection confidence threshold\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (boxes, scores, labels) for detected vehicles and people\n",
    "    \"\"\"\n",
    "    # TODO: Process image for RT-DETR. Use detr_processor, and remember\n",
    "    # to set return_tensors=\"pt\"\n",
    "    inputs = ... # complete\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        # TODO: run the detr_model on our pre-processed inputs\n",
    "        # HINT: remember to unpack (use **inputs)\n",
    "        outputs = ... # complete\n",
    "\n",
    "    # Post-process results\n",
    "    results = detr_processor.post_process_object_detection(\n",
    "        outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Filter for vehicles and people only (person=0, car=2, bus=5, truck=7)\n",
    "    all_boxes = results[0][\"boxes\"]\n",
    "    all_scores = results[0][\"scores\"]\n",
    "    all_labels = results[0][\"labels\"]\n",
    "\n",
    "    vehicle_classes = [0, 2, 5, 7]  # person, car, bus, truck\n",
    "    vehicle_mask = torch.isin(all_labels, torch.tensor(vehicle_classes))\n",
    "\n",
    "    boxes = all_boxes[vehicle_mask]\n",
    "    scores = all_scores[vehicle_mask]\n",
    "    labels = all_labels[vehicle_mask]\n",
    "\n",
    "    # Filter out boxes with high IoU with whole image (dashboard removal)\n",
    "    if len(boxes) > 0:\n",
    "        img_width, img_height = image.size\n",
    "        img_area = img_width * img_height\n",
    "\n",
    "        # Calculate box areas\n",
    "        box_areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "        # IoU with whole image is just box_area / img_area (since box is always inside image)\n",
    "        ious = box_areas / img_area\n",
    "\n",
    "        # Keep boxes with IoU less than threshold\n",
    "        keep_mask = ious < iou_threshold_for_dashboard_removal\n",
    "        boxes = boxes[keep_mask]\n",
    "        scores = scores[keep_mask]\n",
    "        labels = labels[keep_mask]\n",
    "\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_vehicles_and_people(image: Image.Image, boxes: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Segment vehicles and people using SAM2.1 given bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image to process\n",
    "        boxes: Bounding boxes from vehicle detection\n",
    "        \n",
    "    Returns:\n",
    "        List of segmentation masks\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # TODO: Run Sam 2.1 on our image, using the boxes from \n",
    "    # the object detection stage as prompts\n",
    "    inputs = sam_processor(\n",
    "        images=..., # complete by providing our image\n",
    "        input_boxes=[...], # complete. Hint: should be [boxes], not just boxes\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # TODO: Run the SAM model\n",
    "        # HINT: remember to unpack (so use **inputs)\n",
    "        outputs = ... # complete\n",
    "\n",
    "    masks = sam_processor.post_process_masks(\n",
    "        outputs.pred_masks.cpu(), inputs[\"original_sizes\"]\n",
    "    )[0]\n",
    "\n",
    "    # Get IoU scores to select best mask for each box\n",
    "    iou_scores = outputs.iou_scores.cpu()\n",
    "\n",
    "    # Select the best mask for each box based on highest IoU score\n",
    "    best_masks = []\n",
    "    for i in range(masks.shape[0]):\n",
    "        best_mask_idx = torch.argmax(iou_scores[0, i])\n",
    "        best_mask = masks[i, best_mask_idx]\n",
    "        best_masks.append(best_mask)\n",
    "\n",
    "    return best_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label_driving_scene(\n",
    "    images: Union[Image.Image, List[Image.Image]], \n",
    "    detection_threshold: float = 0.3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Complete auto-labeling pipeline for driving scenes.\n",
    "\n",
    "    Args:\n",
    "        images: Single PIL Image or list of PIL Images\n",
    "        detection_threshold: Confidence threshold for vehicle detection\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing results for each image:\n",
    "        {\n",
    "            'image_idx': int,\n",
    "            'time_of_day': str,\n",
    "            'time_confidence': float,\n",
    "            'weather': str,\n",
    "            'weather_confidence': float,\n",
    "            'num_vehicles': int,\n",
    "            'vehicle_boxes': torch.Tensor,\n",
    "            'vehicle_scores': torch.Tensor,\n",
    "            'vehicle_labels': torch.Tensor,\n",
    "            'vehicle_masks': List[torch.Tensor]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Handle single image case\n",
    "    if isinstance(images, Image.Image):\n",
    "        images = [images]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        print(f\"Processing image {idx + 1}/{len(images)}...\")\n",
    "\n",
    "        # Step 1: Classify driving conditions\n",
    "        conditions = classify_driving_conditions(image)\n",
    "\n",
    "        # Step 2: Detect vehicles\n",
    "        boxes, scores, labels = detect_vehicles_and_people(image, detection_threshold)\n",
    "\n",
    "        # Step 3: Segment vehicles\n",
    "        masks = segment_vehicles_and_people(image, boxes)\n",
    "\n",
    "        # Compile results\n",
    "        result = {\n",
    "            \"image_idx\": idx,\n",
    "            \"time_of_day\": conditions[\"time_of_day\"],\n",
    "            \"time_confidence\": conditions[\"time_confidence\"],\n",
    "            \"weather\": conditions[\"weather\"],\n",
    "            \"weather_confidence\": conditions[\"weather_confidence\"],\n",
    "            \"num_vehicles_and_people\": len(boxes),\n",
    "            \"boxes\": boxes,\n",
    "            \"scores\": scores,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"  Found {len(boxes)} objects (vehicles and people).\")\n",
    "        print(\n",
    "            f\"  Time: {conditions['time_of_day']} ({conditions['time_confidence']:.2f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Weather: {conditions['weather']} ({conditions['weather_confidence']:.2f})\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Image and Test Pipeline\n",
    "\n",
    "Let's load a sample driving image and test our complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can use images/one.png, images/two.png, images/three.png or images/four.png\n",
    "image = Image.open(\"../images/one.png\")\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Driving Scene\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline\n",
    "\n",
    "Now let's run our complete auto-labeling pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "results = auto_label_driving_scene([image.convert(\"RGB\")])\n",
    "\n",
    "# Get results for our single image\n",
    "result = results[0]\n",
    "\n",
    "print(\"\\nComplete Pipeline Results:\")\n",
    "print(f\"Time of day: {result['time_of_day']} (confidence: {result['time_confidence']:.3f})\")\n",
    "print(f\"Weather: {result['weather']} (confidence: {result['weather_confidence']:.3f})\")\n",
    "print(f\"Vehicles and people detected: {result['num_vehicles_and_people']}\")\n",
    "print(f\"Segmentation masks generated: {len(result['masks'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's visualize the complete pipeline results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete pipeline results\n",
    "visualize_pipeline_results(\n",
    "    image,\n",
    "    result['time_of_day'],\n",
    "    result['time_confidence'],\n",
    "    result['weather'],\n",
    "    result['weather_confidence'],\n",
    "    result['masks']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the other images as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob(\"../images/*.png\")\n",
    "\n",
    "images = [Image.open(x).convert(\"RGB\") for x in paths]\n",
    "\n",
    "results = auto_label_driving_scene(images)\n",
    "\n",
    "for image, result in zip(images, results):\n",
    "\n",
    "    visualize_pipeline_results(\n",
    "        image,\n",
    "        result['time_of_day'],\n",
    "        result['time_confidence'],\n",
    "        result['weather'],\n",
    "        result['weather_confidence'],\n",
    "        result['masks']\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-vision-preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
