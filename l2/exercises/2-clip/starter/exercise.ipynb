{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First Image Search with CLIP\n",
    "\n",
    "In this notebook, you'll learn how to build a simple image search system using CLIP (Contrastive Language-Image Pre-training) from Hugging Face. CLIP can understand both text and images, making it perfect for searching images using natural language descriptions.\n",
    "\n",
    "## What you'll learn:\n",
    "- Load a pre-trained CLIP model\n",
    "- Process images and text\n",
    "- Find the most similar images to a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Helper functions\n",
    "\n",
    "First, let's define a simple helper function to display images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "\n",
    "def display_grid(images: List[Image.Image], *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrap the make_grid function from torchvision accepting PIL.Image instances\n",
    "    instead of tensors. Every other arguments or keyword arguments is passed\n",
    "    to make_grid.\n",
    "\n",
    "    Images are going to be resized and center-cropped to 256x256 to fit\n",
    "    in the grid.\n",
    "    \"\"\"\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize(256),  # resize shorter side to 256\n",
    "            T.CenterCrop(256),  # center crop to 256x256 square\n",
    "            T.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tensors = [transform(img) for img in images]\n",
    "    grid = torchvision.utils.make_grid(tensors, *args, **kwargs)\n",
    "    _, sub = plt.subplots(dpi=300)\n",
    "    sub.imshow(grid.permute(1, 2, 0))\n",
    "    sub.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Load the Model\n",
    "\n",
    "Now let's import the necessary libraries and load a pre-trained CLIP model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# TODO: load processor and model using .from_pretrained\n",
    "# on respectively CLIPProcessor and CLIPModel\n",
    "processor = ... # complete\n",
    "model = ... # complete\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset\n",
    "\n",
    "Let's load a dataset of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: Load the \"test\" split of the \"xai-org/RealworldQA\" dataset\n",
    "# We use this Realworld Q/A dataset because it is small (765) but\n",
    "# pretty diverse\n",
    "dataset = ... # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what we got. Let's display a few images in a grid\n",
    "images = dataset['image']\n",
    "display_grid(images[:25], nrow=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Image Embeddings\n",
    "\n",
    "Now we'll convert our images into embeddings (numerical representations) that CLIP can work with. These embeddings capture the visual features of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Process images and get embeddings\n",
    "def get_image_embeddings_batch(images, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process images in batches for speed\n",
    "    \"\"\"\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, len(images), batch_size), total=int(np.ceil(len(images) / batch_size))\n",
    "    ):\n",
    "        \n",
    "        # Get a batch of images\n",
    "        batch = images[i : i + batch_size]\n",
    "\n",
    "        # TODO: Run the pre-processor to get the inputs to the CLIP model\n",
    "        # Remember to use return_tensors=\"pt\" and padding=True\n",
    "        inputs = ... # complete\n",
    "\n",
    "        # TODO: Get image features from the model using get_image_features\n",
    "        with torch.no_grad():\n",
    "            # HINT: remember to unpack the inputs with **inputs\n",
    "            image_features = ... # complete\n",
    "\n",
    "        # NOTE: since we will be using cosine similarity, we pre-normalize the embeddings\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        all_embeddings.append(image_features)\n",
    "\n",
    "        if (i + batch_size) % 100 == 0 or (i + batch_size) >= len(images):\n",
    "            print(f\"Processed {min(i + batch_size, len(images))} images...\")\n",
    "    \n",
    "    # Right now the list all_embeddings is a list of batches, we\n",
    "    # collapse it into a list of embeddings\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "\n",
    "print(\"Creating embeddings for all batches of images...\")\n",
    "image_embeddings = get_image_embeddings_batch(images)\n",
    "print(f\"Created embeddings with shape: {image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Search Images with Images\n",
    "\n",
    "Here you will implement an image-to-image search. Given a query image, the function should:\n",
    "\n",
    "1. Compute the embedding of the query image using .get_image_features\n",
    "2. Normalize the image feature\n",
    "3. Compute the cosine similarity. Since the query image features and the\n",
    "    dataset embeddings are already normalized, cosine similarity is now \n",
    "    equivalent to just the matrix multiplication between the query image embeddings\n",
    "    and the image embeddings transposed.\n",
    "4. sort the image similarities in descending order and take the first top_k elements\n",
    "5. return the scores and the top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_images(\n",
    "    image_embeddings: List[torch.Tensor],\n",
    "    query_image: Image.Image,\n",
    "    top_k: int = 5,\n",
    "    plot: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a set of image embeddings and an image query, returns the first top_k matches\n",
    "    with the highest cosine-similarity scores. \n",
    "\n",
    "    The image embeddings are assumed pre-normalized.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Pre-process the input image\n",
    "    # Remember to use return_tensors=\"pt\" and padding=True\n",
    "    # HINT: provide to the processor a list of one element: [query_image]\n",
    "    inputs = ... # complete\n",
    "\n",
    "    # Get image embedding from the model\n",
    "    with torch.no_grad():\n",
    "        # TODO: use get_image_features.\n",
    "        # HINT: remember to unpack the inputs with **inputs\n",
    "        query_image_features = ... # complete\n",
    "\n",
    "    print(\n",
    "        f\"Query image has been transformed to embedding vector of shape {query_image_features[0].shape} ...\"\n",
    "    )\n",
    "\n",
    "    # We use cosine similarity as matching score:\n",
    "    # cosine_similarity = (A · B) / (||A|| × ||B||)\n",
    "    #  so we need to:\n",
    "    # 1 - Normalize query image features\n",
    "    query_image_features = query_image_features / query_image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # 2 - Normalize image embeddings\n",
    "    # (we already did during pre-computation!)\n",
    "\n",
    "    # 3 - Now the formula reduces to just the matrix multiplication\n",
    "    # TODO: do the matrix multiplication. Remember to transpose image_embeddings\n",
    "    # HINT: use torch.matmul on the query image features and the transpose of the image\n",
    "    # embeddings\n",
    "    similarities = ... # complete\n",
    "\n",
    "    # Sort the similarities and get the _indices_ of the first \"top_k\" results\n",
    "    top_indices = similarities[0].argsort(descending=True)[:top_k]\n",
    "\n",
    "    scores = similarities[0][top_indices]\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        for i, score in zip(top_indices, scores):\n",
    "            print(f\"{i} - {score:.3f}\")\n",
    "\n",
    "        display_grid([images[int(i)] for i in top_indices], nrow=5)\n",
    "\n",
    "    return top_indices, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Try Some Searches\n",
    "\n",
    "Now let's test our image-to-image search system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's an image of a street and see if we find others:\n",
    "street_image = images[222]\n",
    "street_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, scores = search_images(image_embeddings, street_image, top_k=5, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Can you explain why the first image is exactly the one we provided as query and has a score of 1?\n",
    "\n",
    "(Since the image we give is part of the dataset, the first result is bound to be that image with a perfect score, because an image is a perfect match to itselfQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = images[0]\n",
    "query_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the image-to-image search again as we did previously\n",
    "... # complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
