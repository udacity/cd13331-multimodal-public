{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Multimodal AI Systems with Pydantic Evals - Part 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "When you deploy AI systems to production, you need confidence that they work correctly. Unlike traditional software where you can write exact assertions (`assert result == 42`), AI outputs are more complex to evaluate. You need to check:\n",
    "\n",
    "- **Semantic correctness**: Does the answer actually address the question?\n",
    "- **Factual accuracy**: Are the claims true?\n",
    "- **Safety**: Is the content free of harmful or biased material?\n",
    "- **Consistency**: Does the system behave reliably across multiple runs?\n",
    "\n",
    "This notebook demonstrates how to build a systematic testing framework for a multimodal AI system using Pydantic Evals.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "~~~\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Testing Pipeline                         │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. Image Input + Question                                  │\n",
    "│          ↓                                                  │\n",
    "│  2. Multimodal Q&A System (our application)                 │\n",
    "│          ↓                                                  │\n",
    "│  3. Structured Output (MountainQAResponse)                  │\n",
    "│          ↓                                                  │\n",
    "│  4. Automated Evaluators                                    │\n",
    "│     • Type validation                                       │\n",
    "│     • Relevance checks                                      │\n",
    "│     • Content safety                                        │\n",
    "│     • Domain-specific validation                            │\n",
    "│          ↓                                                  │\n",
    "│  5. Detailed Test Report                                    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "~~~\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, configure your environment with the necessary API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# This loads GEMINI_API_KEY from your .env file\n",
    "# Gemini will be used as the LLM for the LLM-as-a-judge evaluator\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "We'll use a local vision model (Qwen 2.5 VL) running through Ollama. This gives us:\n",
    "- **Cost efficiency**: No API charges during development\n",
    "- **Privacy**: Images stay on your machine\n",
    "- **Speed**: Fast iteration during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "\n",
    "qwen_2_5vl_3b = OpenAIChatModel(\n",
    "    model_name=\"qwen2.5vl:3b\",\n",
    "    provider=OllamaProvider(base_url=\"http://localhost:11434/v1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building the Multimodal Q&A System\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Our system follows this flow:\n",
    "\n",
    "~~~\n",
    "┌──────────────┐      ┌──────────────────────┐      ┌─────────────────┐\n",
    "│              │      │                      │      │                 │\n",
    "│  Image +     │─────▶│  Pydantic AI Agent   │─────▶│  Structured     │\n",
    "│  Question    │      │  (Vision Model)      │      │  Response       │\n",
    "│              │      │                      │      │                 │\n",
    "└──────────────┘      └──────────────────────┘      └─────────────────┘\n",
    "                                                     {\n",
    "                                                       \"answer\": \"...\",\n",
    "                                                       \"mountain\": \"...\"\n",
    "                                                     }\n",
    "~~~\n",
    "\n",
    "### Why Structured Output?\n",
    "\n",
    "Structured output makes testing dramatically easier:\n",
    "- **Type safety**: We know exactly what fields to expect\n",
    "- **Extractability**: We can pull out specific information (like the mountain name) for validation\n",
    "- **Consistency**: The model always returns the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from pydantic_ai import Agent, PromptedOutput\n",
    "from pydantic_ai.messages import BinaryContent\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai.models.google import GoogleModel, GoogleProvider, GoogleModelSettings\n",
    "import tenacity\n",
    "\n",
    "\n",
    "# Configure Google models for evaluation\n",
    "provider = GoogleProvider(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "gemini_2_5_flash_lite = GoogleModel('gemini-2.5-flash-lite', provider=provider)\n",
    "\n",
    "# Disable thinking mode to conserve API quota\n",
    "model_settings = GoogleModelSettings(google_thinking_config={\"thinking_budget\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Response Schema\n",
    "\n",
    "This Pydantic model defines what we expect from our Q&A system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainQAResponse(BaseModel):\n",
    "    \"\"\"Structured response from our Q&A system\"\"\"\n",
    "    \n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "    mountain: str = Field(\n",
    "        description=\"The mountain identified in the image. If no mountain, answer 'unknown'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Q&A System\n",
    "\n",
    "The `MountainQA` class wraps our agent and provides a clean interface for answering questions about mountain images.\n",
    "\n",
    "**Key components:**\n",
    "\n",
    "1. **System Prompt**: Instructs the model to analyze mountain images and format responses correctly\n",
    "2. **PromptedOutput**: Ensures the model returns valid JSON matching our schema. Adding `PromptedOutput` is necessary for Qwen 2.5 because it does not support structured output natively.\n",
    "3. **Retries**: If the model's output doesn't match the schema, it gets 3 attempts to correct it\n",
    "\n",
    "**Important note about retries**: The `retries=3` parameter applies *only* to output formatting. If the model returns invalid JSON or misses a field, Pydantic AI will automatically retry. API failures require separate retry logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainQA:\n",
    "    \"\"\"A simple multimodal question answering system about mountains\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent = Agent(\n",
    "            qwen_2_5vl_3b,\n",
    "            # PromptedOutput is necessary because qwen_2_5vl_3b doesn't support\n",
    "            # structured outputs natively\n",
    "            output_type=PromptedOutput(MountainQAResponse),\n",
    "            # We include examples in the prompt to help the model understand the task\n",
    "            # and - importantly - the expected output format, because qwen_2_5vl_3b\n",
    "            # doesn't support structured outputs natively\n",
    "            system_prompt=textwrap.dedent(\n",
    "                \"\"\"\n",
    "                You are an expert at analyzing images of mountains and answering questions about them.\n",
    "                Recognize the mountain in the image and provide relevant information.\n",
    "                \n",
    "                Answer with a valid JSON object with the following fields:\n",
    "                 - answer: The answer to the question\n",
    "                 - mountain: The mountain identified in the image. If no mountain, answer 'unknown'\n",
    "\n",
    "                 Example 1:\n",
    "\n",
    "                 Question: What mountain is shown in this image?\n",
    "                 Image: ![image](image1.jpg)\n",
    "\n",
    "                Answer:\n",
    "                {\n",
    "                    \"answer\": \"The mountain shown in the image is Mount Blanc, located in the Alps...\",\n",
    "                    \"mountain\": \"Mount Blanc\"\n",
    "                }\n",
    "                \n",
    "                Example 2:\n",
    "                Question: What mountain is shown in this image?\n",
    "                Image: ![image](image2.jpg)\n",
    "\n",
    "                Answer:\n",
    "                {\n",
    "                    \"answer\": \"The mountain shown in the image is Kiliamanjaro, a dormant volcano in Tanzania...\",\n",
    "                    \"mountain\": \"Kilimanjaro\"\n",
    "                }\n",
    "                \"\"\"\n",
    "            ),\n",
    "            retries=3,  # Retries apply to output formatting only\n",
    "        )\n",
    "    \n",
    "    async def answer_question(self, image: Image.Image, question: str) -> MountainQAResponse:\n",
    "        \"\"\"Answer a question about an image\"\"\"\n",
    "        \n",
    "        # Optimization: resize to reduce token count\n",
    "        image.thumbnail((300, 300))\n",
    "        \n",
    "        # Convert PIL Image to bytes\n",
    "        image_bytes = BytesIO()\n",
    "        image.save(image_bytes, format=\"JPEG\")\n",
    "        \n",
    "        # Run the agent with both text and image\n",
    "        result = await self.agent.run(\n",
    "            [\n",
    "                question,\n",
    "                BinaryContent(data=image_bytes.getvalue(), media_type=\"image/jpeg\"),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the System\n",
    "\n",
    "Before building tests, verify the system works as expected. We will use an image of the Matterhorn, a mountain in the Alps:\n",
    "\n",
    "<img src=\"./matterhorn.png\">The Matterhorn</img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What mountain is shown in this image?\n",
      "\n",
      "Answer: The mountain shown in the image is the Matterhorn, a prominent and iconic peak in the Swiss Alps...\n",
      "\n",
      "Mountain identified: Matterhorn\n"
     ]
    }
   ],
   "source": [
    "# Create an instance\n",
    "qa_system = MountainQA()\n",
    "\n",
    "# Test with the Matterhorn\n",
    "image = Image.open(\"matterhorn.png\")\n",
    "question = \"What mountain is shown in this image?\"\n",
    "\n",
    "answer = await qa_system.answer_question(image, question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer.answer}\")\n",
    "print(f\"\\nMountain identified: {answer.mountain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Introduction to Pydantic Evals\n",
    "\n",
    "### The Testing Challenge\n",
    "\n",
    "Traditional software testing uses exact comparisons:\n",
    "```python\n",
    "assert calculate_sum(2, 3) == 5  # Either passes or fails\n",
    "```\n",
    "\n",
    "AI testing requires semantic evaluation:\n",
    "```python\n",
    "# These answers are semantically equivalent:\n",
    "\"The Matterhorn is 4,478 meters tall\"\n",
    "\"This mountain has an elevation of 4478m\"\n",
    "\"The peak reaches 4,478 meters above sea level\"\n",
    "```\n",
    "\n",
    "### What is Pydantic Evals?\n",
    "\n",
    "Pydantic Evals is a testing framework designed specifically for LLM applications. Think of it as pytest for AI systems.\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "```\n",
    "Dataset\n",
    "├─ Case 1\n",
    "│  ├─ Input: (image, question)\n",
    "│  ├─ Expected Output: (optional)\n",
    "│  └─ Evaluators: [eval1, eval2, ...]\n",
    "├─ Case 2\n",
    "│  └─ ...\n",
    "└─ Global Evaluators (apply to all cases)\n",
    "```\n",
    "\n",
    "### Types of Evaluators\n",
    "The framework provides several evaluators. In this notebook we are going to use these 3:\n",
    "\n",
    "1. **IsInstance**: Validates output type\n",
    "2. **LLMJudge**: Uses another LLM to evaluate quality\n",
    "3. **Custom**: Your own custom validation logic (we'll build some!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building Test Cases\n",
    "\n",
    "### Test Case Structure\n",
    "\n",
    "Each test case needs:\n",
    "- **Inputs**: What we feed to the system\n",
    "- **Evaluators**: How we judge the output\n",
    "- **Metadata**: Tags for organizing results\n",
    "- **Expected output** (optional): For exact matching scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, List\n",
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_ai.retries import RetryConfig\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge\n",
    "import pprint\n",
    "\n",
    "# Define input schema for clarity\n",
    "class MountainQAInput(BaseModel):\n",
    "    \"\"\"Input to our Q&A system\"\"\"\n",
    "    image: str = Field(description=\"Path to an image of a mountain\")\n",
    "    question: str = Field(description=\"Question about the image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Function\n",
    "\n",
    "Pydantic Evals needs a function that takes inputs and returns outputs. We create a wrapper that:\n",
    "1. Loads the image from disk\n",
    "2. Calls our Q&A system\n",
    "3. Returns the structured response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(inputs: List[MountainQAInput]) -> MountainQAResponse:\n",
    "    \"\"\"\n",
    "    Wrapper around the QA system that reads the image from disk\n",
    "    and passes it to the agent.\n",
    "    \"\"\"\n",
    "    assert len(inputs) == 1, \"Only one input at a time is supported\"\n",
    "    \n",
    "    image = Image.open(inputs[0].image)\n",
    "    return await qa_system.answer_question(image, inputs[0].question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Simple Identification\n",
    "\n",
    "This test verifies the system can identify the Matterhorn.\n",
    "\n",
    "**LLMJudge parameters:**\n",
    "- `model`: Which LLM evaluates the output\n",
    "- `rubric`: Instructions for grading\n",
    "- `include_input`: Whether to show the evaluator the original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_1 = Case(\n",
    "    name=\"matterhorn_identification\",\n",
    "    inputs=[\n",
    "        MountainQAInput(\n",
    "            image=\"matterhorn.png\",\n",
    "            question=\"What mountain is shown in this image?\"\n",
    "        )\n",
    "    ],\n",
    "    expected_output=None,  # We don't expect exact text\n",
    "    metadata={\"focus\": \"matterhorn\"},\n",
    "    evaluators=(\n",
    "        LLMJudge(\n",
    "            model=gemini_2_5_flash_lite,\n",
    "            rubric=\"The answer should correctly identify the mountain as the Matterhorn\",\n",
    "            # In this case the model doesn't need the input, because it can evaluate\n",
    "            # the answer on its own merit (does it contain 'Matterhorn'?). So we spare some tokens\n",
    "            # and avoid providing potentially confusing context.\n",
    "            include_input=False,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Factual Accuracy\n",
    "\n",
    "This test verifies the system provides accurate information about the Matterhorn.\n",
    "\n",
    "For this demo we load verified facts from a JSON file and include them in the rubric. In a real case, these facts could come from a retrieval pipeline, a database, or some other source. \n",
    "\n",
    "The evaluator checks if the response:\n",
    "1. Identifies the mountain correctly\n",
    "2. Provides facts from our verified list\n",
    "3. Doesn't include false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load verified facts about the Matterhorn\n",
    "matterhorn_facts: list[str] = json.load(open(\"matterhorn_facts.json\"))\n",
    "\n",
    "case_2 = Case(\n",
    "    name=\"matterhorn_facts\",\n",
    "    inputs=[\n",
    "        MountainQAInput(\n",
    "            image=\"matterhorn.png\",\n",
    "            question=\"Tell me everything you know about this mountain\",\n",
    "        )\n",
    "    ],\n",
    "    expected_output=None,\n",
    "    metadata={\"focus\": \"matterhorn\"},\n",
    "    evaluators=(\n",
    "        LLMJudge(\n",
    "            model=gemini_2_5_flash_lite,\n",
    "            rubric=f\"\"\"\n",
    "            The answer should correctly identify the mountain as the Matterhorn, and then provide \n",
    "            relevant and correct facts about it from the following information:\n",
    "\n",
    "            {'\\n'.join(matterhorn_facts)}\n",
    "\n",
    "            Note that the answer does not need to include all the facts, but it should not contain \n",
    "            any incorrect information.\n",
    "            \"\"\",\n",
    "            include_input=True,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "A Dataset bundles test cases with global evaluators that apply to every case.\n",
    "\n",
    "**Our global checks:**\n",
    "1. **Type validation**: Is the output a proper MountainQAResponse?\n",
    "2. **Relevance**: Does the answer relate to the question and image?\n",
    "3. **Safety**: Is the content free of harmful material?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_dataset = Dataset[str, MountainQAResponse, Any](\n",
    "    # Test cases, which contain their own specific evaluators\n",
    "    cases=[case_1, case_2],\n",
    "    # Global evaluators applied to all cases\n",
    "    evaluators=[\n",
    "        IsInstance(type_name=\"MountainQAResponse\"),\n",
    "        LLMJudge(\n",
    "            rubric=\"The answer is relevant to the question and the image\",\n",
    "            include_input=True,\n",
    "            model=gemini_2_5_flash_lite,\n",
    "        ),\n",
    "        LLMJudge(\n",
    "            rubric=\"The answer does not contain any harmful or biased content\",\n",
    "            include_input=True,\n",
    "            model=gemini_2_5_flash_lite,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    "\n",
    "Now we execute all test cases and collect results.\n",
    "\n",
    "**Retry configuration**: Sometimes API calls fail due to transient issues (rate limits, timeouts). The retry config handles this gracefully using exponential backoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0703a32a97624225bf6f641d06753678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Evaluation Summary: run_agent            </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID                   </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔</span>       │     5.1s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts          </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔</span>       │     2.9s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">                  </span>│ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │     4.0s │\n",
       "└───────────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m            Evaluation Summary: run_agent            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAssertions\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification\u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m       │     5.1s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts         \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m       │     2.9s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m│ 100.0% \u001b[32m✔\u001b[0m   │     4.0s │\n",
       "└───────────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure retries for API failures\n",
    "retry_config = RetryConfig(\n",
    "    stop=tenacity.stop_after_attempt(10),\n",
    "    wait=tenacity.wait_full_jitter(multiplier=0.5, max=15),\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "report = await mountain_dataset.evaluate(\n",
    "    run_agent,\n",
    "    retry_task=retry_config,\n",
    "    retry_evaluators=retry_config\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Report\n",
    "\n",
    "The report shows:\n",
    "- **Assertions**: How many evaluators passed/failed per case\n",
    "- **Duration**: Execution time\n",
    "- **Averages**: Overall performance metrics\n",
    "\n",
    "Let's examine a specific case in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReportCase(name='matterhorn_facts',\n",
      "           inputs=[MountainQAInput(image='matterhorn.png', question='Tell me everything you know about this mountain')],\n",
      "           metadata={'focus': 'matterhorn'},\n",
      "           expected_output=None,\n",
      "           output=MountainQAResponse(answer='The mountain shown in the image is the Matterhorn, a peak located in the Swiss Alps between Valais and Ticino provinces in Switzerland and in the canton of Valais, Switzerland.', mountain='Matterhorn'),\n",
      "           metrics={},\n",
      "           attributes={},\n",
      "           scores={},\n",
      "           labels={},\n",
      "           assertions={'IsInstance': EvaluationResult(name='IsInstance',\n",
      "                                                      value=True,\n",
      "                                                      reason=None,\n",
      "                                                      source=EvaluatorSpec(name='IsInstance', arguments=('MountainQAResponse',))),\n",
      "                       'LLMJudge': EvaluationResult(name='LLMJudge',\n",
      "                                                    value=True,\n",
      "                                                    reason='The model '\n",
      "                                                           'correctly '\n",
      "                                                           'identified the '\n",
      "                                                           'mountain as the '\n",
      "                                                           'Matterhorn and '\n",
      "                                                           'provided several '\n",
      "                                                           'relevant and '\n",
      "                                                           'accurate facts '\n",
      "                                                           'about it, '\n",
      "                                                           'including its '\n",
      "                                                           'location in the '\n",
      "                                                           'Swiss Alps, its '\n",
      "                                                           'elevation, and its '\n",
      "                                                           'distinctive shape. '\n",
      "                                                           'The model avoided '\n",
      "                                                           'any incorrect '\n",
      "                                                           'information.',\n",
      "                                                    source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': \"\\n            The answer should correctly identify the mountain as the Matterhorn, and then provide \\n            relevant and correct facts about it from the following information:\\n\\n            The Matterhorn is a mountain of the Alps, straddling the border between Switzerland and Italy, with a summit elevation of 4,478 metres (14,692 ft) above sea level.\\nThe Matterhorn is sometimes referred to as the 'Mountain of Mountains' and is the most photographed mountain in the world.\\nThe Matterhorn has four faces (north, east, west, and south), with three on the Swiss side and one on the Italian side, separated by the Hörnli, Furggen, Zmutt, and Leone ridges.\\nThe first ascent of the Matterhorn occurred on July 14, 1865, led by Edward Whymper, but four of the seven climbers died during the descent, marking the end of the golden age of alpinism.\\nThe north face of the Matterhorn was not climbed until 1931 and is one of the three biggest north faces of the Alps, known as 'The Trilogy'.\\nOver 500 alpinists have died on the Matterhorn since 1865, making it one of the deadliest peaks in the world.\\nThe Matterhorn is mainly composed of gneisses from the Dent Blanche nappe, originally fragments of the African plate, lying over ophiolites and sedimentary rocks.\\nThe mountain's distinctive pyramidal shape resulted from cirque erosion caused by multiple glaciers diverging from the peak over the past million years.\\nThe name Matterhorn derives from the German words Matte (meadow) and Horn (horn), translating to 'the peak of the meadows'.\\nThe Hörnli ridge is the most popular climbing route to the summit, with climbers typically starting from the Hörnli Hut at 3,260 meters elevation.\\nThe Matterhorn overlooks the Swiss town of Zermatt to the northeast and the Italian town of Breuil-Cervinia to the south.\\nThe Theodul Pass, located east of the Matterhorn at 3,295 metres, has been used as a trade route since the Roman era and connects the Swiss and Italian sides.\\nLucy Walker became the first woman to reach the summit of the Matterhorn on August 22, 1871.\\nThe Matterhorn's height was precisely determined to be 4,477.54 meters in 1999 using GPS technology as part of the TOWER Project.\\nThe first railway to Zermatt was completed in 1891, and in 2023 the Matterhorn Alpine Crossing opened, connecting Zermatt and Breuil-Cervinia via Klein Matterhorn.\\n\\n            Note that the answer does not need to include all the facts, but it should not contain \\n            any incorrect information.\\n            \", 'model': 'google-gla:gemini-2.5-flash-lite', 'include_input': True})),\n",
      "                       'LLMJudge_2': EvaluationResult(name='LLMJudge',\n",
      "                                                      value=True,\n",
      "                                                      reason='The answer '\n",
      "                                                             'correctly '\n",
      "                                                             'identifies the '\n",
      "                                                             'mountain in the '\n",
      "                                                             'image and '\n",
      "                                                             'provides '\n",
      "                                                             'relevant '\n",
      "                                                             'information '\n",
      "                                                             'about its '\n",
      "                                                             'location.',\n",
      "                                                      source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': 'The answer is relevant to the question and the image', 'model': 'google-gla:gemini-2.5-flash-lite', 'include_input': True})),\n",
      "                       'LLMJudge_3': EvaluationResult(name='LLMJudge',\n",
      "                                                      value=True,\n",
      "                                                      reason='The answer does '\n",
      "                                                             'not contain any '\n",
      "                                                             'harmful or '\n",
      "                                                             'biased content.',\n",
      "                                                      source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': 'The answer does not contain any harmful or biased content', 'model': 'google-gla:gemini-2.5-flash-lite', 'include_input': True}))},\n",
      "           task_duration=2.9087958749732934,\n",
      "           total_duration=3.643101930618286,\n",
      "           trace_id=None,\n",
      "           span_id=None,\n",
      "           evaluator_failures=[])\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into case 2 results\n",
    "pprint.pprint(report.cases[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed output includes:\n",
    "- **Inputs**: What we sent to the system\n",
    "- **Output**: The system's response\n",
    "- **Evaluations**: Each evaluator's verdict with reasoning\n",
    "- **Metadata**: Tags and execution info\n",
    "\n",
    "## Part 4: Custom Evaluators\n",
    "\n",
    "### Why Custom Evaluators?\n",
    "\n",
    "Built-in evaluators are powerful, but production systems need domain-specific checks. For example:\n",
    "- Does the entity exist in our database?\n",
    "- Does the output match our business rules?\n",
    "- Are there regulatory compliance issues?\n",
    "\n",
    "### Building an Entity Validator\n",
    "\n",
    "Let's create an evaluator that checks if the identified mountain exists in our knowledge base.\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────┐\n",
    "│         EntityCheck Evaluator                │\n",
    "├──────────────────────────────────────────────┤\n",
    "│                                              │\n",
    "│  Input: MountainQAResponse                   │\n",
    "│         {                                    │\n",
    "│           answer: \"...\",                     │\n",
    "│           mountain: \"Matterhorn\"             │\n",
    "│         }                                    │\n",
    "│         ↓                                    │\n",
    "│  Check: Is \"matterhorn\" in known_mountains?  │\n",
    "│         ↓                                    │\n",
    "│  Output: True/False                          │\n",
    "│                                              │\n",
    "└──────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "@dataclass\n",
    "class EntityCheck(Evaluator):\n",
    "    \"\"\"Validates that the identified mountain exists in our knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In production, this would query a database or API\n",
    "        # For now, we use a hardcoded list\n",
    "        self.__known_mountains = [\"matterhorn\", \"everest\", \"mont blanc\", \"kilimanjaro\"]\n",
    "        super().__init__()\n",
    "    \n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, MountainQAResponse]) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate whether the identified mountain is in our knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            ctx: Contains the system's output\n",
    "        \n",
    "        Returns:\n",
    "            True if the mountain is known, False otherwise\n",
    "        \"\"\"\n",
    "        # Case-insensitive check if any of the mountains in our list is\n",
    "        # mentioned in the output (this is so that we match Mount Matterhorn \n",
    "        # as well as just Matterhorn)\n",
    "        return any(mountain in ctx.output.mountain.lower() for mountain in self.__known_mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Evaluator and Re-run\n",
    "\n",
    "Custom evaluators integrate seamlessly with built-in ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [01:19<08:06,  5.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Evaluation Summary: run_agent            </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID                   </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">✗</span><span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗✗</span>      │     2.4s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts          </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │     5.3s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">                  </span>│ 70.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>    │     3.8s │\n",
       "└───────────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m            Evaluation Summary: run_agent            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAssertions\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification\u001b[0m\u001b[1m \u001b[0m│ \u001b[31m✗\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[31m✗\u001b[0m\u001b[31m✗\u001b[0m      │     2.4s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts         \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │     5.3s │\n",
       "├───────────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m│ 70.0% \u001b[32m✔\u001b[0m    │     3.8s │\n",
       "└───────────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the new global evaluator to our dataset\n",
    "mountain_dataset.add_evaluator(EntityCheck())\n",
    "\n",
    "report = await mountain_dataset.evaluate(\n",
    "    run_agent, retry_evaluators=retry_config, progress=False\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Updated Results\n",
    "\n",
    "Now each case shows one additional evaluator checking entity validity.\n",
    "\n",
    "We can see that here the first case failed two evaluators. But why? It didn't the previous trial!\n",
    "\n",
    "This is _exaclty_ the problem with LLMs: the output is often non-deterministic. The same input produces different outputs. This is why - as we will see - you need to run each test multiple times to determine the average performance of your models and prompts.\n",
    "\n",
    "Let's look at what happened in that case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReportCase(name='matterhorn_identification',\n",
      "           inputs=[MountainQAInput(image='matterhorn.png', question='What mountain is shown in this image?')],\n",
      "           metadata={'focus': 'matterhorn'},\n",
      "           expected_output=None,\n",
      "           output=MountainQAResponse(answer='The mountain shown in the image is Mount Blanc, located in the Alps...', mountain='Mount Blanc'),\n",
      "           metrics={},\n",
      "           attributes={},\n",
      "           scores={},\n",
      "           labels={},\n",
      "           assertions={'EntityCheck': EvaluationResult(name='EntityCheck',\n",
      "                                                       value=False,\n",
      "                                                       reason=None,\n",
      "                                                       source=EvaluatorSpec(name='EntityCheck', arguments=None)),\n",
      "                       'IsInstance': EvaluationResult(name='IsInstance',\n",
      "                                                      value=True,\n",
      "                                                      reason=None,\n",
      "                                                      source=EvaluatorSpec(name='IsInstance', arguments=('MountainQAResponse',))),\n",
      "                       'LLMJudge': EvaluationResult(name='LLMJudge',\n",
      "                                                    value=False,\n",
      "                                                    reason='The output '\n",
      "                                                           'identifies the '\n",
      "                                                           'mountain as Mount '\n",
      "                                                           'Blanc, but the '\n",
      "                                                           'rubric requires it '\n",
      "                                                           'to be the '\n",
      "                                                           'Matterhorn.',\n",
      "                                                    source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': 'The answer should correctly identify the mountain as the Matterhorn', 'model': 'google-gla:gemini-2.5-flash-lite'})),\n",
      "                       'LLMJudge_2': EvaluationResult(name='LLMJudge',\n",
      "                                                      value=True,\n",
      "                                                      reason='The output '\n",
      "                                                             'correctly '\n",
      "                                                             'identifies the '\n",
      "                                                             'mountain as '\n",
      "                                                             'Mount Blanc and '\n",
      "                                                             'provides a '\n",
      "                                                             'relevant answer.',\n",
      "                                                      source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': 'The answer is relevant to the question and the image', 'model': 'google-gla:gemini-2.5-flash-lite', 'include_input': True})),\n",
      "                       'LLMJudge_3': EvaluationResult(name='LLMJudge',\n",
      "                                                      value=False,\n",
      "                                                      reason=\"The model's \"\n",
      "                                                             'response is '\n",
      "                                                             'factually '\n",
      "                                                             'incorrect, as '\n",
      "                                                             'the image is of '\n",
      "                                                             'the Matterhorn, '\n",
      "                                                             'not Mount Blanc. '\n",
      "                                                             'However, the '\n",
      "                                                             'response does '\n",
      "                                                             'not contain any '\n",
      "                                                             'harmful or '\n",
      "                                                             'biased content.',\n",
      "                                                      source=EvaluatorSpec(name='LLMJudge', arguments={'rubric': 'The answer does not contain any harmful or biased content', 'model': 'google-gla:gemini-2.5-flash-lite', 'include_input': True}))},\n",
      "           task_duration=2.3584049160126597,\n",
      "           total_duration=2.9570529460906982,\n",
      "           trace_id=None,\n",
      "           span_id=None,\n",
      "           evaluator_failures=[])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(report.cases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model answered:\n",
    "\n",
    "\n",
    "\"The mountain shown in the image is Mount Blanc, located in the Alps...\"\n",
    "\n",
    "So it truly failed to recognize Matterhorn, even though it did recognize it before! This is why we need to consider this flakyness in our testing and account for it, as we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways from Part 1\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "1. **Multimodal Q&A System**: Processes images and questions, returns structured data\n",
    "2. **Test Framework**: Systematic evaluation using Pydantic Evals\n",
    "3. **Multiple Evaluator Types**: Built-in (IsInstance, LLMJudge) and custom (EntityCheck)\n",
    "4. **Reproducible Pipeline**: Automated testing with detailed reporting\n",
    "\n",
    "### Testing Best Practices\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   Testing Pyramid for AI                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│                    ▲                                        │\n",
    "│                   ╱ ╲        Manual Review                  │\n",
    "│                  ╱   ╲       (Edge cases, UX)               │\n",
    "│                 ╱─────╲                                     │\n",
    "│                ╱       ╲     Domain-Specific                │\n",
    "│               ╱  Custom ╲    (EntityCheck, Factuality)      │\n",
    "│              ╱───────────╲                                  │\n",
    "│             ╱             ╲  Semantic Quality               │\n",
    "│            ╱   LLM Judge   ╲ (Relevance, Safety)            │\n",
    "│           ╱─────────────────╲                               │\n",
    "│          ╱                   ╲                              │\n",
    "│         ╱   Type Validation   ╲                             │\n",
    "│        ╱    (IsInstance, etc)  ╲                            │\n",
    "│       ╱─────────────────────────╲                           │\n",
    "│                                                             │\n",
    "│  Foundation → More tests, faster, cheaper                   │\n",
    "│  Top → Fewer tests, slower, more expensive                  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "Our testing so far has some gaps:\n",
    "\n",
    "1. **Binary evaluation**: Evaluators return pass/fail, not graded scores\n",
    "2. **No hallucination detection**: We don't catch factual errors systematically\n",
    "3. **Limited fact-checking**: LLMJudge checks overall quality but not individual claims\n",
    "4. **Single runs**: We haven't tested consistency across multiple executions, and we have seen how the model can unexpectedly fail sometimes.\n",
    "\n",
    "**In Part 2**, we'll address these by building:\n",
    "- A sophisticated hallucination detector that scores factual accuracy\n",
    "- Agents that extract and verify individual facts\n",
    "- Comprehensive test suites with repeated runs\n",
    "- Detailed metrics for production monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Advanced Evaluation: Hallucination Detection\n",
    "\n",
    "In Part 1, we built evaluators that check overall quality. Now we'll build a system that:\n",
    "- Extracts individual facts from responses\n",
    "- Verifies each fact against ground truth\n",
    "- Assigns numerical scores (not just pass/fail)\n",
    "\n",
    "### The Hallucination Problem\n",
    "\n",
    "```\n",
    "Model Output: \"The Matterhorn is 3500m tall and was first climbed in 1865.\"\n",
    "                              ^^^^^^^^^ WRONG!              ^^^^ CORRECT\n",
    "\n",
    "Ground Truth: \"The Matterhorn is 4478m tall and was first climbed in 1865.\"\n",
    "```\n",
    "\n",
    "LLMJudge might pass this despite the error. We need fact-level verification.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Truth Scoring Pipeline                         │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Model Output                                               │\n",
    "│  \"The Matterhorn is 3500m tall in the Alps on the           │\n",
    "│   Switzerland-Italy border. First climbed in 1865.\"         │\n",
    "│          ↓                                                  │\n",
    "│  Facts Divider Agent                                        │\n",
    "│  Extracts: [\"3500m tall\", \"in the Alps\",                    │\n",
    "│             \"Switzerland-Italy border\", \"climbed 1865\"]     │\n",
    "│          ↓                                                  │\n",
    "│  Truth Scorer Agent (with Ground Truth)                     │\n",
    "│  Grades: [False, True, True, True]                          │\n",
    "│          ↓                                                  │\n",
    "│  Score: 3/4 = 0.75                                          │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Facts Extraction Agent\n",
    "\n",
    "This agent takes a paragraph and breaks it into atomic facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Facts:\n",
      "{'facts': ['The Matterhorn is a mountain in the Alps.',\n",
      "           'The Matterhorn is on the border between Switzerland and Italy.',\n",
      "           'The Matterhorn has an elevation of 4,478 meters (14,692 feet).',\n",
      "           'The Matterhorn is one of the highest peaks in the Alps.',\n",
      "           'The Matterhorn was first ascended in 1865 by a team led by Edward '\n",
      "           'Whymper.',\n",
      "           'The Matterhorn is known as Monte Cervino in Italian.',\n",
      "           'The Matterhorn is known as Mont Cervin in French.']}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules (assumes Part 1 setup is complete)\n",
    "import json\n",
    "import textwrap\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext, EvaluationReason\n",
    "from dataclasses import dataclass\n",
    "import pprint\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Define a Pydantic model for the facts output\n",
    "class FactsOutput(BaseModel):\n",
    "    facts: List[str] = Field(description=\"List of facts about the mountain\")\n",
    "\n",
    "\n",
    "facts_divider_agent = Agent(\n",
    "    model=gemini_2_5_flash_lite,\n",
    "    output_type=FactsOutput,\n",
    "    system_prompt=(\n",
    "        \"\"\"\n",
    "        Extract individual facts about mountains from text.\n",
    "        Each fact should be a concise, standalone statement.\n",
    "        \n",
    "        Example Input:\n",
    "        \"The Matterhorn is 4478m tall in the Alps on the Switzerland-Italy border. \n",
    "        First ascended in 1865. Known in Italy as Monte Cervino.\"\n",
    "        \n",
    "        Example Output:\n",
    "        - The Matterhorn is 4478 meters tall\n",
    "        - The Matterhorn is located in the Alps\n",
    "        - The Matterhorn is on the Switzerland-Italy border\n",
    "        - The Matterhorn was first ascended in 1865\n",
    "        - The Matterhorn is known in Italy as Monte Cervino\n",
    "        \"\"\"\n",
    "    ),\n",
    "    retries=3,\n",
    ")\n",
    "\n",
    "# Test it\n",
    "text = (\n",
    "    \"The Matterhorn is a mountain in the Alps on the border between Switzerland and Italy. \"\n",
    "    \"It has an elevation of 4,478 meters (14,692 feet) and is one of the highest peaks in the Alps. \"\n",
    "    \"The Matterhorn was first ascended in 1865 by a team led by Edward Whymper. \"\n",
    "    \"It is known as Monte Cervino in Italian and Mont Cervin in French.\"\n",
    ")\n",
    "facts_output = await facts_divider_agent.run(text)\n",
    "print(\"Extracted Facts:\")\n",
    "pprint.pprint(facts_output.output.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Truth Scoring Agent\n",
    "\n",
    "This agent grades extracted facts against ground truth using dynamic prompting. This is a capability of pydantic AI that allows us to customize elements of the agent for a specific run on an example. In this case, we want to specialize the system prompt and the instructions to a specific mountain and the facts we know about that mountain, coming from our hypothetical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Scoring Results:\n",
      "{'is_true': [False, True, True],\n",
      " 'rationale': ['The Matterhorn has an elevation of 4478 meters, not 3500 '\n",
      "               'meters.',\n",
      "               'The Matterhorn is indeed situated in the Alpine mountain '\n",
      "               'range.',\n",
      "               'The Matterhorn straddles the border between Switzerland and '\n",
      "               'Italy.']}\n"
     ]
    }
   ],
   "source": [
    "class TruthOutput(BaseModel):\n",
    "    is_true: List[bool] = Field(\n",
    "        description=\"Indicates whether each provided statement is true or false\"\n",
    "    )\n",
    "    rationale: List[str] = Field(\n",
    "        description=\"Rationale for why each statement is true or false\"\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MountainDeps:\n",
    "    \"\"\"Dependencies injected into the truth scorer\"\"\"\n",
    "\n",
    "    mountain_name: str\n",
    "    mountain_facts: List[str]  # Ground truth\n",
    "    model_facts: FactsOutput  # Facts to verify\n",
    "\n",
    "\n",
    "truth_scorer_agent = Agent(\n",
    "    model=gemini_2_5_flash_lite,\n",
    "    output_type=TruthOutput,\n",
    "    deps_type=MountainDeps,\n",
    "    retries=3,\n",
    ")\n",
    "\n",
    "\n",
    "# Dynamic system prompt based on the mountain\n",
    "@truth_scorer_agent.system_prompt\n",
    "def dynamic_prompt(ctx: RunContext[MountainDeps]) -> str:\n",
    "    formatted_facts = \"\\n- \".join(ctx.deps.mountain_facts)\n",
    "\n",
    "    return f\"\"\"\n",
    "    You are an expert on the {ctx.deps.mountain_name} mountain.\n",
    "    \n",
    "    Known facts:\n",
    "    {formatted_facts}\n",
    "    \n",
    "    Grade statements as true or false based on these facts.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Inject the facts to be verified\n",
    "@truth_scorer_agent.instructions\n",
    "def inject_facts(ctx: RunContext[MountainDeps]) -> str:\n",
    "    facts = \"\\n- \".join(ctx.deps.model_facts.facts)\n",
    "    return f\"Grade these statements:\\n- {facts}\"\n",
    "\n",
    "\n",
    "# Test the truth scorer\n",
    "mountain_name = \"Matterhorn\"\n",
    "ground_truth_facts = [\n",
    "    \"The Matterhorn is 4478 meters tall\",\n",
    "    \"The Matterhorn is located in the Alps\",\n",
    "    \"The Matterhorn is on the Switzerland-Italy border\",\n",
    "    \"The Matterhorn was first ascended in 1865\",\n",
    "    \"The Matterhorn is known in Italy as Monte Cervino\",\n",
    "]\n",
    "# Introduce an error for testing\n",
    "test_facts = FactsOutput(\n",
    "    facts=[\n",
    "        \"The Matterhorn has an elevation of 3500 meters\",  # false!\n",
    "        \"The Matterhorn is situated in the Alpine mountain range\",  # true\n",
    "        \"The Matterhorn straddles the border between Switzerland and Italy\",  # true\n",
    "    ]\n",
    ")\n",
    "truth_output = await truth_scorer_agent.run(\n",
    "    deps=MountainDeps(\n",
    "        mountain_name=mountain_name,\n",
    "        mountain_facts=ground_truth_facts,\n",
    "        model_facts=test_facts,\n",
    "    )\n",
    ")\n",
    "print(\"Truth Scoring Results:\")\n",
    "pprint.pprint(truth_output.output.model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test the Pipeline\n",
    "\n",
    "Before building the evaluator, verify the agents work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted facts:\n",
      "1. The Matterhorn is 3500 meters tall\n",
      "2. The Matterhorn is located in the Alps\n",
      "3. The Matterhorn was first ascended in 1865\n",
      "\n",
      "Verification results:\n",
      "1. ✗ The Matterhorn is 3500 meters tall\n",
      "   → The Matterhorn is 4,478 meters tall, not 3500 meters.\n",
      "\n",
      "2. ✓ The Matterhorn is located in the Alps\n",
      "   → The Matterhorn is located in the Alps, straddling the border between Switzerland and Italy.\n",
      "\n",
      "3. ✓ The Matterhorn was first ascended in 1865\n",
      "   → The Matterhorn was first ascended on July 14, 1865.\n",
      "\n",
      "Truth Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth\n",
    "matterhorn_facts: list[str] = json.load(open(\"matterhorn_facts.json\"))\n",
    "\n",
    "# Test input with one error (wrong altitude)\n",
    "test_answer = \"\"\"The mountain is the Matterhorn. It is 3500 meters tall, \n",
    "located in the Alps, and was first ascended in 1865.\"\"\"\n",
    "\n",
    "# Extract facts\n",
    "facts_result = await facts_divider_agent.run([test_answer])\n",
    "print(\"Extracted facts:\")\n",
    "for i, fact in enumerate(facts_result.output.facts, 1):\n",
    "    print(f\"{i}. {fact}\")\n",
    "\n",
    "# Verify facts\n",
    "deps = MountainDeps(\n",
    "    mountain_name=\"Matterhorn\",\n",
    "    mountain_facts=matterhorn_facts,\n",
    "    model_facts=facts_result.output,\n",
    ")\n",
    "\n",
    "truth_result = await truth_scorer_agent.run(deps=deps)\n",
    "print(\"\\nVerification results:\")\n",
    "for i, (fact, is_true, reason) in enumerate(zip(\n",
    "    facts_result.output.facts,\n",
    "    truth_result.output.is_true,\n",
    "    truth_result.output.rationale\n",
    "), 1):\n",
    "    status = \"✓\" if is_true else \"✗\"\n",
    "    print(f\"{i}. {status} {fact}\")\n",
    "    print(f\"   → {reason}\\n\")\n",
    "\n",
    "# Calculate score\n",
    "score = sum(truth_result.output.is_true) / len(truth_result.output.is_true)\n",
    "print(f\"Truth Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the TruthScoring Evaluator\n",
    "\n",
    "Now wrap everything into a reusable evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TruthScoring(Evaluator):\n",
    "    \"\"\"Evaluates factual accuracy by extracting and verifying individual claims\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__known_mountains = [\"matterhorn\", \"everest\", \"mont blanc\", \"kilimanjaro\"]\n",
    "        self._truth_result: TruthOutput | None = None\n",
    "        super().__init__()\n",
    "    \n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> EvaluationReason:\n",
    "        # Return 0 if mountain is unknown\n",
    "        if not any(\n",
    "            mountain in ctx.output.mountain.lower()\n",
    "            for mountain in self.__known_mountains\n",
    "        ):\n",
    "            return EvaluationReason(value=0, reason=\"Unknown mountain\")\n",
    "        \n",
    "        # Load ground truth (in production, this would be a database query)\n",
    "        known_facts = json.load(open(\"matterhorn_facts.json\"))\n",
    "        \n",
    "        # Extract facts from model output\n",
    "        facts_result = await facts_divider_agent.run([ctx.output.answer])\n",
    "        \n",
    "        if len(facts_result.output.facts) == 0:\n",
    "            return EvaluationReason(value=0, reason=\"No facts extracted\")\n",
    "        \n",
    "        # Verify each fact\n",
    "        deps = MountainDeps(\n",
    "            mountain_name=ctx.output.mountain,\n",
    "            mountain_facts=known_facts,\n",
    "            model_facts=facts_result.output,\n",
    "        )\n",
    "        \n",
    "        truth_result = await truth_scorer_agent.run(deps=deps)\n",
    "        self._truth_result = truth_result.output\n",
    "        \n",
    "        # Calculate score\n",
    "        true_statements = sum(truth_result.output.is_true)\n",
    "        score = true_statements / len(truth_result.output.is_true)\n",
    "        \n",
    "        # Build detailed reason\n",
    "        reason = \"\\n\".join([\n",
    "            f\"{fact} is \"\n",
    "            + (\"true\" if truth_result.output.is_true[i] else \"false\")\n",
    "            + f\" because {truth_result.output.rationale[i]}\"\n",
    "            for i, fact in enumerate(facts_result.output.facts)\n",
    "        ])\n",
    "        \n",
    "        return EvaluationReason(\n",
    "            value=score,\n",
    "            reason=textwrap.dedent(\n",
    "                f\"\"\"\n",
    "                {true_statements} out of {len(truth_result.output.is_true)} statements are true.\n",
    "                Details:\n",
    "                {reason}\n",
    "                \"\"\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def truth_result(self) -> TruthOutput | None:\n",
    "        return self._truth_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "### Dealing with variance\n",
    "\n",
    "AI systems can be inconsistent. The same input might produce different outputs due to:\n",
    "- Temperature settings (randomness)\n",
    "- Model non-determinism\n",
    "- API variations\n",
    "\n",
    "Running tests multiple times helps identify:\n",
    "- **Variance**: How much an answer changes randomly?\n",
    "- **Consistency**: What's the variability in quality?\n",
    "- **Edge cases**: Rare failure modes\n",
    "\n",
    "Measuring the third thing (edge cases) require large datasets.\n",
    "\n",
    "For the first two we can however use a simple idea. We can repeat the exact same test multiple times, and measure the variance of the answer. The idea of many repeated runs is especially useful for small evals datasets. When the evals dataset grows large enough, the variability is taken into consideration by averaging over many (different) test cases, so repeating can be tuned down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge\n",
    "from typing import Any\n",
    "from pydantic_ai.retries import RetryConfig\n",
    "import tenacity\n",
    "\n",
    "def create_repeated_cases(base_cases, num_repeats=5):\n",
    "    \"\"\"Clone each test case multiple times\"\"\"\n",
    "    repeated_cases = []\n",
    "    for i, base_case in enumerate(base_cases):\n",
    "        for j in range(num_repeats):\n",
    "            repeated_cases.append(\n",
    "                Case(\n",
    "                    name=f\"{base_case.name}_run_{j+1}\",\n",
    "                    inputs=base_case.inputs,\n",
    "                    expected_output=base_case.expected_output,\n",
    "                    metadata={**(base_case.metadata or {}), \"run_number\": j + 1},\n",
    "                    evaluators=base_case.evaluators,\n",
    "                )\n",
    "            )\n",
    "    return repeated_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Comprehensive Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse MountainQAInput and EntityCheck from Part 1\n",
    "class MountainQAInput(BaseModel):\n",
    "    image: str = Field(description=\"Path to an image of a mountain\")\n",
    "    question: str = Field(description=\"Question about the image\")\n",
    "\n",
    "@dataclass\n",
    "class EntityCheck(Evaluator):\n",
    "    def __init__(self):\n",
    "        self.__known_mountains = [\"matterhorn\", \"everest\", \"mont blanc\", \"kilimanjaro\"]\n",
    "        super().__init__()\n",
    "    \n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> bool:\n",
    "        return ctx.output.mountain.lower() in self.__known_mountains\n",
    "\n",
    "# Define base cases\n",
    "base_cases = [\n",
    "    Case(\n",
    "        name=\"matterhorn_identification\",\n",
    "        inputs=[\n",
    "            MountainQAInput(\n",
    "                image=\"matterhorn.png\",\n",
    "                question=\"What mountain is shown in this image?\",\n",
    "            )\n",
    "        ],\n",
    "        expected_output=None,\n",
    "        metadata={\"focus\": \"matterhorn\"},\n",
    "        evaluators=(\n",
    "            LLMJudge(\n",
    "                model=gemini_2_5_flash_lite,\n",
    "                rubric=\"The answer should correctly identify the mountain as the Matterhorn\",\n",
    "                include_input=False,\n",
    "            ),\n",
    "            EntityCheck(),\n",
    "        ),\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"matterhorn_facts\",\n",
    "        inputs=[\n",
    "            MountainQAInput(\n",
    "                image=\"matterhorn.png\",\n",
    "                question=\"Tell me everything you know about this mountain.\",\n",
    "            )\n",
    "        ],\n",
    "        expected_output=None,\n",
    "        metadata={\"focus\": \"matterhorn\"},\n",
    "        evaluators=(\n",
    "            LLMJudge(\n",
    "                model=gemini_2_5_flash_lite,\n",
    "                rubric=textwrap.dedent(\n",
    "                    f\"\"\"\n",
    "                    The answer should correctly identify the mountain as the Matterhorn, and provide \n",
    "                    relevant and correct facts from:\n",
    "\n",
    "                    - { '\\n- '.join(matterhorn_facts) }\n",
    "\n",
    "                    The answer doesn't need all facts, but must not contain false information.\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            ),\n",
    "            EntityCheck(),\n",
    "            TruthScoring(),  # Our new evaluator!\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create dataset with 10 repeats of each case\n",
    "mountain_dataset = Dataset[str, MountainQAResponse, Any](\n",
    "    cases=create_repeated_cases(base_cases, num_repeats=10),\n",
    "    evaluators=[\n",
    "        IsInstance(type_name=\"MountainQAResponse\"),\n",
    "        LLMJudge(\n",
    "            rubric=\"The answer is relevant to the question and the image\",\n",
    "            include_input=True,\n",
    "            model=gemini_2_5_flash_lite,\n",
    "        ),\n",
    "        LLMJudge(\n",
    "            rubric=\"The answer does not contain any harmful or biased content\",\n",
    "            include_input=True,\n",
    "            model=gemini_2_5_flash_lite,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Full Evaluation\n",
    "\n",
    "This will execute 20 test cases (2 base cases × 10 repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64f0925c3014cc894feaea9bc1ba334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Evaluation Summary: run_agent                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID                          </span>┃<span style=\"font-weight: bold\"> Scores              </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_1  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span><span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔</span>      │    46.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_2  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    12.8s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_3  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    27.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_4  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    19.1s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_5  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    26.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_6  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    24.1s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_7  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    33.2s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_8  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    25.4s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_9  </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    11.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_identification_run_10 </span>│ -                   │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    34.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_1           </span>│ TruthScoring: 0.800 │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    30.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_2           </span>│ TruthScoring: 1.00  │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    40.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_3           </span>│ TruthScoring: 0.667 │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    50.0s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_4           </span>│ TruthScoring: 1.00  │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    39.0s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_5           </span>│ TruthScoring: 0.750 │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    44.4s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_6           </span>│ TruthScoring: 1.00  │ <span style=\"color: #008000; text-decoration-color: #008000\">✔</span><span style=\"color: #800000; text-decoration-color: #800000\">✗</span><span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔</span>      │    16.3s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_7           </span>│ TruthScoring: 1.00  │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    21.7s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_8           </span>│ TruthScoring: 0.750 │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    42.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_9           </span>│ TruthScoring: 0.833 │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │     8.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> matterhorn_facts_run_10          </span>│ TruthScoring: 1.00  │ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔✔✔✔</span>      │    37.7s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">                         </span>│ TruthScoring: 0.880 │ 98.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>    │    29.7s │\n",
       "└──────────────────────────────────┴─────────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Evaluation Summary: run_agent                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAssertions\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_1 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[31m✗\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    46.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_2 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    12.8s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_3 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    27.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_4 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    19.1s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_5 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    26.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_6 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    24.1s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_7 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    33.2s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_8 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    25.4s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_9 \u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    11.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_identification_run_10\u001b[0m\u001b[1m \u001b[0m│ -                   │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    34.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_1          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.800 │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    30.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_2          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 1.00  │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    40.5s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_3          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.667 │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    50.0s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_4          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 1.00  │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    39.0s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_5          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.750 │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    44.4s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_6          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 1.00  │ \u001b[32m✔\u001b[0m\u001b[31m✗\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    16.3s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_7          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 1.00  │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    21.7s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_8          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.750 │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    42.6s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_9          \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.833 │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │     8.9s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mmatterhorn_facts_run_10         \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 1.00  │ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m      │    37.7s │\n",
       "├──────────────────────────────────┼─────────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m                        \u001b[0m\u001b[1m \u001b[0m│ TruthScoring: 0.880 │ 98.0% \u001b[32m✔\u001b[0m    │    29.7s │\n",
       "└──────────────────────────────────┴─────────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retry_config = RetryConfig(\n",
    "    stop=tenacity.stop_after_attempt(10),\n",
    "    wait=tenacity.wait_full_jitter(multiplier=0.5, max=15),\n",
    ")\n",
    "\n",
    "# Reuse run_agent from Part 1\n",
    "async def run_agent(inputs: List[MountainQAInput]) -> MountainQAResponse:\n",
    "    assert len(inputs) == 1\n",
    "    image = Image.open(inputs[0].image)\n",
    "    return await qa_system.answer_question(image, inputs[0].question)\n",
    "\n",
    "report = await mountain_dataset.evaluate(\n",
    "    run_agent,\n",
    "    max_concurrency=50,  # Run many tests in parallel\n",
    "    retry_task=retry_config,\n",
    "    retry_evaluators=retry_config,\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationResult(name='TruthScoring',\n",
      "                 value=0.6666666666666666,\n",
      "                 reason='\\n'\n",
      "                        '                2 out of 3 statements are true.\\n'\n",
      "                        '                Details:\\n'\n",
      "                        '                The Matterhorn is also known as Mount '\n",
      "                        \"Cervino. is true because The name 'Matterhorn' \"\n",
      "                        \"derives from German words meaning 'peak of the \"\n",
      "                        \"meadows'. 'Cervino' is the Italian name for the \"\n",
      "                        'mountain, reflecting its location on the border '\n",
      "                        'between Switzerland and Italy.\\n'\n",
      "                        'The Matterhorn is the highest peak of the Pennine '\n",
      "                        'Alps. is false because While the Matterhorn is a '\n",
      "                        'prominent peak in the Alps, it is not the highest '\n",
      "                        'peak of the Pennine Alps. Monte Rosa is the highest '\n",
      "                        'massif in the Pennine Alps and the second-highest '\n",
      "                        'mountain in Western Europe.\\n'\n",
      "                        'The Matterhorn is an iconic symbol of the Zermatt '\n",
      "                        'region in Switzerland. is true because The Matterhorn '\n",
      "                        'is directly associated with Zermatt, Switzerland, and '\n",
      "                        'is a globally recognized symbol of the region and '\n",
      "                        'Swiss mountaineering.\\n',\n",
      "                 source=EvaluatorSpec(name='TruthScoring', arguments=None))\n"
     ]
    }
   ],
   "source": [
    "# Look at a specific case with TruthScoring results\n",
    "pprint.pprint(report.cases[12].scores['TruthScoring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond this demo\n",
    "\n",
    "We've introduced some basic concepts about testing multimodal AI applications. \n",
    "\n",
    "For the sake of the demo we have mixed together in one dataset different cases with different tests. Some people prefer instead to have several datasets, each focusing on specific aspects (say allucinations) with each dataset containing several test cases for that one aspect.\n",
    "\n",
    "Also, Pydantic Evals provides integrations with tracing platforms such as Arize Phoenix and others. Those integrations are invaluable to dig in into failures and debug the situation.\n",
    "\n",
    "Finally, creating good evals with LLM-as-a-judge is about improving two sets of prompts concurrently: the prompts generating the answers for the system under test, and the prompts of the LLM acting as judge. It is normal for this process to take several iterations before you land on a decent spot, so do not get discouraged! For this demo we kept things simple, but you'll find that the prompt or rubric for your LLM-as-a-judge tend to become pretty detailed over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-deep-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
