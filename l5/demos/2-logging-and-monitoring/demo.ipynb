{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging and monitoring a multimodal application\n",
    "\n",
    "In this notebook, we'll take a minimal Gradio chatbot that uses Pydantic AI to interact with Google Gemini, and we will add logging and monitoring.\n",
    "\n",
    "First, set up your credentials:\n",
    "\n",
    "```bash\n",
    "cp env.example .env\n",
    "```\n",
    "\n",
    "Then add your Gemini API key to `.env`:\n",
    "\n",
    "```yaml\n",
    "GEMINI_API_KEY=[your key]\n",
    "```\n",
    "\n",
    "Load the credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"GEMINI_API_KEY not found. Please check your .env file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Gradio application\n",
    "\n",
    "This is a minimal Gradio application defining a chatbot, using Pydantic AI to interact with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.messages import BinaryContent\n",
    "from pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n",
    "import gradio as gr\n",
    "\n",
    "import uuid\n",
    "import filetype\n",
    "\n",
    "from rate_limiting import rate_limit\n",
    "\n",
    "\n",
    "# These are the MIME types supported by Gemini as of today\n",
    "# (see https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash)\n",
    "SUPPORTED_MIME_TYPES = [\n",
    "    # Image\n",
    "    \"image/png\",\n",
    "    \"image/jpeg\",\n",
    "    \"image/webp\",\n",
    "    # Text\n",
    "    \"application/pdf\",\n",
    "    \"text/plain\",\n",
    "    # Video\n",
    "    \"video/x-flv\",\n",
    "    \"video/quicktime\",\n",
    "    \"video/mpeg\",\n",
    "    \"video/mpegs\",\n",
    "    \"video/mpg\",\n",
    "    \"video/mp4\",\n",
    "    \"video/webm\",\n",
    "    \"video/wmv\",\n",
    "    \"video/3gpp\",\n",
    "    # Audio\n",
    "    \"audio/x-aac\",\n",
    "    \"audio/flac\",\n",
    "    \"audio/mp3\",\n",
    "    \"audio/m4a\",\n",
    "    \"audio/mpeg\",\n",
    "    \"audio/mpga\",\n",
    "    \"audio/mp4\",\n",
    "    \"audio/ogg\",\n",
    "    \"audio/pcm\",\n",
    "    \"audio/wav\",\n",
    "    \"audio/x-wav\",\n",
    "    \"audio/webm\",\n",
    "]\n",
    "\n",
    "\n",
    "class ChatSession:\n",
    "    \n",
    "    def __init__(self, agent: Agent):\n",
    "\n",
    "        # unique identifier for the session\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create a Pydantic AI agent with Gemini\n",
    "        self.agent = agent\n",
    "        \n",
    "    # This function will be called by Gradio when the user sends a message\n",
    "    async def chat(\n",
    "        self, message, history, past_messages\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            message: dict with 'text' and optional 'files' keys\n",
    "            history: Gradio's chat history (for display only, we ignore it)\n",
    "            past_messages: Pydantic AI's message history (the one we actually use)\n",
    "        \"\"\"\n",
    "        prompt_parts = []\n",
    "\n",
    "        # See if the input coming from gradio (i.e., from the user)\n",
    "        # has a text part\n",
    "        if message.get(\"text\"):\n",
    "\n",
    "            prompt_parts.append(message[\"text\"])\n",
    "\n",
    "        # See if the input coming from gradio (i.e., from the user)\n",
    "        # has any files attached\n",
    "        if message.get(\"files\"):\n",
    "\n",
    "            for file_path in message[\"files\"]:\n",
    "\n",
    "                # Check that we have a valid media type\n",
    "                kind = filetype.guess(file_path)\n",
    "\n",
    "                if (\n",
    "                    kind is None  # not recognized\n",
    "                    or not kind.mime  # no MIME type\n",
    "                    or kind.mime not in SUPPORTED_MIME_TYPES\n",
    "                ):\n",
    "\n",
    "                    # Not a recognized media type, skip it\n",
    "                    raise IOError(\n",
    "                        f\"Unsupported file type: {file_path} ({kind.mime if kind else 'unknown'})\"\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        file_bytes = f.read()\n",
    "\n",
    "                    # Add the file content as BinaryContent to the prompt parts\n",
    "                    # Pydantic AI will handle uploading it to Gemini\n",
    "                    # We also specify the media type (MIME type)\n",
    "                    # Note: for large files, you will need to use the File API\n",
    "                    # Here we do direct upload for simplicity\n",
    "                    prompt_parts.append(\n",
    "                        BinaryContent(data=file_bytes, media_type=kind.mime)\n",
    "                    )\n",
    "\n",
    "        if not prompt_parts:\n",
    "            raise ValueError(\"Please provide a message or at least one file.\")\n",
    "\n",
    "        # Here we use await, since agent.run is an async method\n",
    "        # This signals to python that this is a potentially long-running operation\n",
    "        # (e.g., waiting for a response from the AI model) and\n",
    "        # it should allow other tasks to run in the meantime, while we wait\n",
    "        result = await self.agent.run(prompt_parts, message_history=past_messages)\n",
    "\n",
    "        # After we finished waiting, we return both the response AND the updated history\n",
    "        return result.output, result.all_messages()\n",
    "\n",
    "\n",
    "class GradioWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper around ChatSession to handle errors in Gradio\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chat_session: ChatSession):\n",
    "        self.chat_session = chat_session\n",
    "    \n",
    "    # Apply rate limiting to our chat function\n",
    "    # (say 120 calls per minute, i.e., 2 calls per second on average)\n",
    "    @rate_limit(calls=120, period=60)\n",
    "    async def chat_gradio(self, message, history, past_messages):\n",
    "        \"\"\"\n",
    "        Wrapper around chat() to catch and display errors in Gradio apps.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            return await self.chat_session.chat(message, history, past_messages)\n",
    "        except Exception as e:\n",
    "\n",
    "            raise gr.Error(f\"Error: {str(e)}\")\n",
    "\n",
    "agent = Agent(\n",
    "    model=GoogleModel(\"gemini-2.5-flash-lite\"),\n",
    "    # You typically want to set a much more specific system prompt, depending on\n",
    "    # your application. Here we keep it generic.\n",
    "    system_prompt=\"You are a helpful AI assistant. Be concise and friendly.\",\n",
    "    # Good practice: limit the max response tokens to avoid excessive usage\n",
    "    # You might also want to consider limiting the thinking budget\n",
    "    # (a token is ~0.75 words of English, so 1024 tokens is about 750 words)\n",
    "    model_settings=GoogleModelSettings(max_response_tokens=1024),\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "gradio_chat_session = GradioWrapper(ChatSession(agent))\n",
    "\n",
    "# State to hold Pydantic AI's message history\n",
    "past_messages_state = gr.State([])\n",
    "\n",
    "# Create the chatbot interface\n",
    "demo = gr.ChatInterface(\n",
    "    # fn is the function that drives the chatbot\n",
    "    # For us, it is the chat method\n",
    "    # NOTE: gradio automatically recognizes async functions and handles them appropriately\n",
    "    fn=gradio_chat_session.chat_gradio,\n",
    "    type=\"messages\",\n",
    "    # Enable multimodal inputs\n",
    "    multimodal=True,\n",
    "    title=\"Multimodal AI Chatbot\",\n",
    "    description=\"Chat with Gemini! Upload images, audio, or video files along with your questions.\",\n",
    "    # This is the field where the user types messages or uploads files\n",
    "    textbox=gr.MultimodalTextbox(\n",
    "        # Allow to upload multiple files of specified types\n",
    "        file_count=\"multiple\",\n",
    "        # Allow images, audio, and video files\n",
    "        file_types=[\"image\", \"audio\", \"video\", \"text\", \".pdf\"],\n",
    "        # Placeholder text\n",
    "        placeholder=\"Type a message or upload files (images, audio, video, text, pdf)...\",\n",
    "    ),\n",
    "    # This is where the chat history is displayed\n",
    "    chatbot=gr.Chatbot(\n",
    "        height=300,\n",
    "        show_copy_button=True,\n",
    "        placeholder=\"üëã Hello! I'm your AI assistant. Upload media files or just chat with me!\",\n",
    "        type=\"messages\",\n",
    "    ),\n",
    "    # Need to manage state for Pydantic AI's message history\n",
    "    additional_inputs=[past_messages_state],  # Add the state as input\n",
    "    additional_outputs=[past_messages_state],  # Add the state as output\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument our chatbot\n",
    "\n",
    "Now let's modify this code by adding our instrumentation. We will use Phoenix Arize for this, since it provides a good integration with Jupyter.\n",
    "\n",
    "In production you will actually deploy a tracing server (either Phoenix or something else) or use commercial platforms for this. Here for convenience we do everything inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "Phoenix UI: http://localhost:6006/\n",
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import phoenix as px\n",
    "\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from openinference.instrumentation.pydantic_ai import OpenInferenceSpanProcessor\n",
    "import uuid\n",
    "import filetype\n",
    "\n",
    "\n",
    "# Launch Phoenix with persistence\n",
    "session = px.launch_app()\n",
    "print(f\"Phoenix UI: {session.url}\")\n",
    "\n",
    "# We define a processing pipeline for the output of our\n",
    "# tool:\n",
    "#                        pydantic AI\n",
    "#.                           |\n",
    "#                            v\n",
    "#                   OpenInferenceSpanProcessor \n",
    "# [translate from pydantic AI to the OTLP format that Phoenix understands]\n",
    "#.                           |\n",
    "#.                           v\n",
    "#           SimpleSpanProcessor with OTLPSpanExported \n",
    "#      [send immediately the span to Phoenix in OTLP format]\n",
    "tracer_provider = TracerProvider()\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# Translate pydantic AI -> Open Telemetry format (OTLP)\n",
    "tracer_provider.add_span_processor(OpenInferenceSpanProcessor())\n",
    "\n",
    "# Configure exporter to send to Phoenix\n",
    "exporter = OTLPSpanExporter(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))\n",
    "\n",
    "# Get tracer for manual span creation\n",
    "# (this is the equivalent of getting a logger in python, but for Phoenix)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "\n",
    "def add_media_link_to_span(file_path: str, media_type: str, index: int):\n",
    "    \"\"\"Save uploaded file and add link to span for reference.\"\"\"\n",
    "    from pathlib import Path\n",
    "    import shutil\n",
    "\n",
    "    current_span = trace.get_current_span()\n",
    "    if not current_span:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create uploads directory if it doesn't exist\n",
    "        uploads_dir = Path(\"./uploaded_media\")\n",
    "        uploads_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Copy file with a unique name\n",
    "        source_path = Path(file_path)\n",
    "        timestamp = str(uuid.uuid4())[:8]\n",
    "        dest_path = uploads_dir / f\"{timestamp}_{source_path.name}\"\n",
    "        shutil.copy(file_path, dest_path)\n",
    "\n",
    "        # Add file metadata to span with absolute file:// URL\n",
    "        absolute_path = dest_path.resolve()\n",
    "        current_span.set_attribute(\n",
    "            f\"input.{media_type}.{index}.url\", f\"file://{absolute_path}\"\n",
    "        )\n",
    "        current_span.set_attribute(\n",
    "            f\"input.{media_type}.{index}.filename\", source_path.name\n",
    "        )\n",
    "        current_span.set_attribute(\n",
    "            f\"input.{media_type}.{index}.size_bytes\", source_path.stat().st_size\n",
    "        )\n",
    "    except Exception:\n",
    "        # If file save fails, silently skip\n",
    "        pass\n",
    "\n",
    "\n",
    "class GradioWrapperWithTracing:\n",
    "\n",
    "    def __init__(self, chat_session: ChatSession):\n",
    "        \n",
    "        self.chat_session = chat_session\n",
    "\n",
    "        # Enable instrumentation in the agent\n",
    "        self.chat_session.agent.instrument = True\n",
    "\n",
    "        # This is key: it allows us to keep every message within the \n",
    "        # same chat organized in the same trace\n",
    "        self.conversation_span = tracer.start_span(\n",
    "            \"conversation\",\n",
    "            attributes={\"session.id\": chat_session.session_id}\n",
    "        )\n",
    "\n",
    "    @rate_limit(calls=120, period=60)\n",
    "    async def chat_gradio(self, message, history, past_messages):\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Nests this span under the existing conversation span            \n",
    "            with tracer.start_as_current_span(\n",
    "                \"chat_turn\",\n",
    "                context=trace.set_span_in_context(self.conversation_span),\n",
    "                # Here we can add custom attributes\n",
    "                attributes={\"turn_number\": len(history) // 2 + 1}\n",
    "            ):\n",
    "                \n",
    "                # If there are files, add them to the span for reference\n",
    "                for index, file_path in enumerate(message.get(\"files\", [])):\n",
    "\n",
    "                    # Check that we have a valid media type\n",
    "                    kind = filetype.guess(file_path)\n",
    "                    if not kind:\n",
    "                        continue\n",
    "\n",
    "                    # Add media link to span\n",
    "                    add_media_link_to_span(file_path, kind.mime, index)\n",
    "                \n",
    "                # Call the real chatbot\n",
    "                return await self.chat_session.chat(message, history, past_messages)\n",
    "            \n",
    "        except Exception as e:\n",
    "\n",
    "            raise gr.Error(f\"Error: {str(e)}\")\n",
    "\n",
    "gradio_wrapper_with_tracing = GradioWrapperWithTracing(ChatSession(agent))\n",
    "\n",
    "# NOTE:\n",
    "# This part is identical to before\n",
    "past_messages_state = gr.State([])\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=gradio_wrapper_with_tracing.chat_gradio,\n",
    "    type=\"messages\",\n",
    "    multimodal=True,\n",
    "    title=\"Multimodal AI Chatbot\",\n",
    "    description=\"Chat with Gemini! Upload images, audio, or video files along with your questions.\",\n",
    "    textbox=gr.MultimodalTextbox(\n",
    "        file_count=\"multiple\",\n",
    "        file_types=[\"image\", \"audio\", \"video\", \"text\"],\n",
    "        placeholder=\"Type a message or upload files (images, audio, video, text)...\",\n",
    "    ),\n",
    "    chatbot=gr.Chatbot(\n",
    "        height=300,\n",
    "        show_copy_button=True,\n",
    "        placeholder=\"Hello! I'm your AI assistant. Upload media files or just chat with me!\",\n",
    "        type=\"messages\",\n",
    "    ),\n",
    "    additional_inputs=[past_messages_state],\n",
    "    additional_outputs=[past_messages_state],\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1656a5ca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.view(height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-logging-and-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
