{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building a Real Estate Description System with Evaluations\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You work for a real estate company that wants to automatically generate property descriptions from images. Your task is to:\n",
    "\n",
    "1. Build a system that analyzes property images and generates descriptions with category labels\n",
    "2. Create evaluators to ensure descriptions are accurate and appropriate\n",
    "3. Test consistency by running the system multiple times\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Build a multimodal AI system with structured output\n",
    "- Create custom evaluators for domain-specific validation\n",
    "- Use LLMJudge for semantic evaluation\n",
    "- Run repeated tests to measure consistency\n",
    "\n",
    "## Setup\n",
    "\n",
    "Copy `../env.example` to `.env`:\n",
    "```bash\n",
    "cp ../env.example .env\n",
    "```\n",
    "then add your Gemini API key by editing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert os.getenv('GEMINI_API_KEY'), \"Please set your GEMINI_API_KEY in the .env file\"\n",
    "\n",
    "# Only needed on the Udacity workspace. Comment this out if running on another system.\n",
    "os.environ['HF_HOME'] = '/voc/data/huggingface'\n",
    "os.environ['OLLAMA_MODELS'] = '/voc/data/ollama/cache'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['PATH'] = f\"/voc/data/ollama/bin:/voc/data/ffmpeg/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/voc/data/ollama/lib:/voc/data/ffmpeg/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# Local vision model for generation, Gemini for evaluation\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "from pydantic_ai.models.google import GoogleModel, GoogleProvider, GoogleModelSettings\n",
    "\n",
    "qwen_2_5vl_3b = OpenAIChatModel(\n",
    "    model_name=\"qwen2.5vl:3b\",\n",
    "    provider=OllamaProvider(base_url=\"http://localhost:11434/v1\"),\n",
    ")\n",
    "\n",
    "provider = GoogleProvider(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "gemini_2_5_flash_lite = GoogleModel('gemini-2.5-flash-lite', provider=provider)\n",
    "model_settings = GoogleModelSettings(google_thinking_config={\"thinking_budget\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Real Estate Description System\n",
    "\n",
    "### Define the Output Schema\n",
    "\n",
    "We want structured output with:\n",
    "- A marketing-friendly description\n",
    "- A category (interior, exterior, or utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class PictureDescription(BaseModel):\n",
    "    \"\"\"Structured real-estate picture description from image analysis\"\"\"\n",
    "\n",
    "    # TODO: Add two fields:\n",
    "    # 1) description: a marketing-friendly description of the picture, as a string\n",
    "    # 2) category: one of 'interior', 'exterior', 'utilities'. \n",
    "    # **NOTE**: Keep this a str, DO NOT USE Literal, we will check the model \n",
    "    # output with a separate evaluator.\n",
    "\n",
    "    description: ... #complete\n",
    "    \n",
    "    # NOTE: here we could use Literal for stronger typing. In that case pydantic AI would\n",
    "    # validate the output and raise an error if the output is not one of the expected values\n",
    "    # (after the retries if we set them). However, during testing we want to see how the model\n",
    "    # behaves in this regard, and have a chance to improve our prompts to increase the probability\n",
    "    # of success. So we set this to str now, NOT Literal. In production, after prompt optimization, \n",
    "    # we would switch to using Literal.\n",
    "    category: ... #complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Agent\n",
    "\n",
    "Build the agent that analyzes images and generates descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, PromptedOutput\n",
    "from pydantic_ai.messages import BinaryContent\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import textwrap\n",
    "\n",
    "\n",
    "class RealEstateDescriptionGenerator:\n",
    "    \"\"\"System for generating descriptions for real-estate pictures\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # TODO: Create an Agent with:\n",
    "        # - model: qwen_2_5vl_3b\n",
    "        # - output_type: PromptedOutput(PictureDescription) - PromptedOutput is needed because\n",
    "        #   qwen doesn't support native structured output\n",
    "        # - system_prompt: Instructions for analyzing the images (see below)\n",
    "        # - retries: 3 (to handle occasional model errors in producing valid JSON)\n",
    "\n",
    "        # SOLUTION:\n",
    "        self.agent = Agent(\n",
    "            ..., #complete. HINT: use the qwen_2_5vl_3b *variable* (NOT a string)\n",
    "            output_type=..., #complete\n",
    "            system_prompt=textwrap.dedent(\n",
    "                \"\"\"\n",
    "                You are an expert at analyzing real-estate pictures and writing compelling descriptions.\n",
    "\n",
    "                Analyze the real-estate picture and provide:\n",
    "                - A clear, marketing-friendly description (2-3 sentences). Highlight the best features\n",
    "                  and avoid mentioning negative aspects.\n",
    "                - The picture category between \"interior\" (for the interior of the home), \n",
    "                  \"exterior\" (for the exterior of the home), or \"utilities\" (for images of utilities \n",
    "                  like HVAC, water heater, electrical panel, etc).\n",
    "                \n",
    "                Return valid JSON with these fields:\n",
    "                - description: The property description\n",
    "                - category: The property category (one of \"interior\", \"exterior\", \"utilities\")\n",
    "\n",
    "                Example:\n",
    "                {\n",
    "                    \"description\": \"A spacious living room with large windows allowing plenty of natural light.\",\n",
    "                    \"category\": \"interior\",\n",
    "                }\n",
    "                \"\"\"\n",
    "            ),\n",
    "            retries=..., #complete\n",
    "        )\n",
    "\n",
    "    async def describe(self, image: Image.Image) -> PictureDescription:\n",
    "        \"\"\"Generate description for a real-estate image\"\"\"\n",
    "\n",
    "        # Resize to reduce token count\n",
    "        image.thumbnail((300, 300))\n",
    "\n",
    "        # Convert PIL Image to bytes\n",
    "        image_bytes = BytesIO()\n",
    "        image.save(image_bytes, format=\"JPEG\")\n",
    "\n",
    "        # TODO: Run the agent with the image by calling await self.agent.run\n",
    "        # HINT: Pass [BinaryContent(data=image_bytes.getvalue(), media_type=\"image/jpeg\")]\n",
    "        result = ... #complete\n",
    "\n",
    "        return result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the System\n",
    "\n",
    "Test the system with a sample image before building evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the system\n",
    "desc_generator = RealEstateDescriptionGenerator()\n",
    "\n",
    "# TODO: Load the \"../images/living_room/living_room_1.jpg\" image \n",
    "# and test the system by calling desc_generator.describe(image)\n",
    "# HINT: Use Image.open(\"../images/living_room/living_room_1.jpg\")\n",
    "test_image = ... #complete\n",
    "result = await ... #complete\n",
    "\n",
    "display(test_image)\n",
    "\n",
    "print(f\"Description: {textwrap.fill(result.description)}\")\n",
    "print(f\"\\nCategory: {result.category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Evaluators\n",
    "\n",
    "### Evaluation Dataset Context\n",
    "\n",
    "In a real deployment, we would have separate evaluation datasets for different room types (living rooms, bedrooms, kitchens) and property features (exteriors, backyards, utilities). For this exercise, we focus on living room images.\n",
    "\n",
    "### Create Room-Specific Evaluators\n",
    "\n",
    "For living room pictures, we want:\n",
    "- The model to mention the room type (living room, family room, lounge) when appropriate\n",
    "- The category to be correctly identified as \"interior\"\n",
    "\n",
    "We'll build a custom evaluator to check the category and an LLMJudge to validate the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge, Evaluator, EvaluatorContext\n",
    "from typing import Any\n",
    "import glob\n",
    "\n",
    "\n",
    "class RealEstateCategoryValidator(Evaluator):\n",
    "    \"\"\"Custom evaluator to validate the picture category vs the ground truth\"\"\"\n",
    "\n",
    "    def __init__(self, category_ground_truth: str):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.category_ground_truth = category_ground_truth\n",
    "\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> bool:\n",
    "        \"\"\"Check if the category is valid\"\"\"\n",
    "\n",
    "        # TODO: Extract the model output from ctx\n",
    "        # HINT: ctx.output\n",
    "        model_result = ... # complete\n",
    "        \n",
    "        # TODO: check if the category matches the ground truth\n",
    "        # HINT: the ground truth is self.category_ground_truth, so\n",
    "        # just test if model_result.category is equal to self.category_ground_truth\n",
    "        match = ... # complete\n",
    "\n",
    "        return match\n",
    "\n",
    "\n",
    "# Input schema\n",
    "class RealEstatePictureInput(BaseModel):\n",
    "    image: str = Field(description=\"Path to real estate picture\")\n",
    "\n",
    "\n",
    "# TODO: Add two evaluators:\n",
    "# 1. RealEstateCategoryValidator(\"living_room\") - to check if category is valid\n",
    "# 2. LLMJudge with the following rubric:\n",
    "# \"\"\"\n",
    "# If the description mentions a room, it should be \"living room\" or equivalent expressions. \n",
    "# It is fine if the description does not mention a room at all, or it uses generic terms\n",
    "# like \"room\" or \"space\". However, it should not mention other rooms \n",
    "# like \"kitchen\" or \"bedroom\".\n",
    "# \"\"\"\n",
    "# HINT: For LLMJudge, use:\n",
    "# - model=gemini_2_5_flash_lite\n",
    "# - include_input=False (the judge does not need to see the image)\n",
    "\n",
    "test_cases = []\n",
    "\n",
    "living_room_images = glob.glob(\"../images/living_room/*.jpg\")\n",
    "\n",
    "for img_path in living_room_images:\n",
    "    test_cases.append(\n",
    "        Case(\n",
    "            name=f\"living_room_{os.path.basename(img_path)}\",\n",
    "            inputs=[RealEstatePictureInput(image=img_path)],\n",
    "            expected_output=None,\n",
    "            metadata={\"room\": \"living_room\"},\n",
    "            evaluators=(\n",
    "                ..., # complete\n",
    "                ..., # complete\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset with Global Evaluators\n",
    "\n",
    "We create a dataset that tests consistency by running each test case multiple times. This helps identify if the system produces stable results.\n",
    "\n",
    "Global evaluators apply to all test cases and include:\n",
    "- Type validation (IsInstance)\n",
    "- Factual accuracy check (LLMJudge verifying features are visible in the image)\n",
    "- Content moderation (LLMJudge checking for inappropriate content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeated_cases(base_cases, num_repeats=5):\n",
    "    \"\"\"Clone each test case multiple times to test consistency\"\"\"\n",
    "    repeated_cases = []\n",
    "    for i, base_case in enumerate(base_cases):\n",
    "        for j in range(num_repeats):\n",
    "            repeated_cases.append(\n",
    "                Case(\n",
    "                    name=f\"{base_case.name}_run_{j+1}\",\n",
    "                    inputs=base_case.inputs,\n",
    "                    expected_output=base_case.expected_output,\n",
    "                    metadata={**(base_case.metadata or {}), \"run_number\": j + 1},\n",
    "                    evaluators=base_case.evaluators,\n",
    "                )\n",
    "            )\n",
    "    return repeated_cases\n",
    "\n",
    "\n",
    "# TODO: Create a Dataset with:\n",
    "# - cases: Use create_repeated_cases(test_cases, num_repeats=5)\n",
    "# - evaluators: Add global evaluators that apply to all cases:\n",
    "#   1. IsInstance(type_name=\"PictureDescription\") to check the output type\n",
    "#   2. LLMJudge to check if features are actually visible in the image. This is the rubric:\n",
    "# \"\"\"\n",
    "# The description could contain objective and subjective elements.\n",
    "# For the objective facts in the description, they should be things that are at least partially \n",
    "# visible or implied by the image. When subjectivity is involved, favor the interpretation\n",
    "# in the description, unless clearly contradicted by the image.\n",
    "# \"\"\"\n",
    "# HINT: Remember to add include_input=True to the LLMJudge so it can see the image\n",
    "\n",
    "# SOLUTION:\n",
    "product_dataset = Dataset[str, PictureDescription, Any](\n",
    "    cases=...,  # complete\n",
    "    evaluators=[\n",
    "        ...,  # complete\n",
    "        ...,  # complete\n",
    "        # Basic moderation check\n",
    "        LLMJudge(\n",
    "            model=gemini_2_5_flash_lite,\n",
    "            rubric=\"The output should not contain any inappropriate or offensive content\",\n",
    "            include_input=False,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "### Create Wrapper Function and Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.retries import RetryConfig\n",
    "import tenacity\n",
    "\n",
    "# TODO: Create a wrapper function that:\n",
    "# 1. Takes a list of PictureDescriptionInput\n",
    "# 2. Loads the image from the path\n",
    "# 3. Calls desc_generator.describe(image)\n",
    "# 4. Returns the PictureDescription\n",
    "\n",
    "# HINT: Look at how run_agent was implemented in the demo\n",
    "\n",
    "async def run_product_system(inputs: List[RealEstatePictureInput]) -> PictureDescription:\n",
    "    \"\"\"Wrapper around the product description system\"\"\"\n",
    "    assert len(inputs) == 1, \"Only one input at a time\"\n",
    "    \n",
    "    # TODO: Open image\n",
    "    # HINT: use Image.open on inputs[0].image\n",
    "    image = ...,  # complete\n",
    "\n",
    "    # TODO: Call desc_generator.describe on the image and return the result\n",
    "    return await ...  # complete\n",
    "\n",
    "# Configure retries\n",
    "retry_config = RetryConfig(\n",
    "    stop=tenacity.stop_after_attempt(5),\n",
    "    wait=tenacity.wait_full_jitter(multiplier=0.5, max=10),\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "report = await product_dataset.evaluate(\n",
    "    run_product_system,\n",
    "    retry_task=retry_config,\n",
    "    retry_evaluators=retry_config,\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Print a specific result just for illustration\n",
    "pprint.pprint(report.cases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-multimodal-evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
